{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f6fed6",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow numpy matplotlib torch torchvision pandas scikit-learn opencv-python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4d98e",
   "metadata": {},
   "source": [
    "## custom loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f120025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[x_ref_input, eta_exp_input],\n",
    "        outputs={\"prediction\": x, \"x_ref_passthrough\": x_ref_input}\n",
    "    )\n",
    "\n",
    "    # Return both the prediction and the input x_ref (as passthrough)\n",
    "    #model = Model(inputs=[x_ref_input, eta_exp_input], outputs=[x, x_ref_input])\n",
    "\n",
    "    # model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        output_dict = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "        preds = output_dict[\"prediction\"]\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def masked_mse_only_on_diff(y_true, y_pred):\n",
    "    # `y_pred` is just the output from 'prediction'\n",
    "    # we will pull `x_ref` from the model's second output via `add_loss`\n",
    "\n",
    "    # Use Keras backend to store global reference\n",
    "    x_ref_tensor = tf.keras.backend.get_value(tf.keras.backend.learning_phase())  # placeholder\n",
    "    raise NotImplementedError(\"You can't access the other output here directly\")\n",
    "\n",
    "# This is just to show that Keras doesn't allow cross-output access in a pure loss function\n",
    "# So we use a custom training step instead — see next\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        (x_ref, eta_exp), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([x_ref, eta_exp], training=True)\n",
    "            y_pred = outputs[\"prediction\"]\n",
    "            x_ref_passthrough = outputs[\"x_ref_passthrough\"]\n",
    "\n",
    "            # Calculate intersection mask\n",
    "            epsilon = 1e-2\n",
    "            mask = tf.cast(tf.abs(y_true - x_ref_passthrough) > epsilon, tf.float32)\n",
    "            sq_diff = tf.square(y_true - y_pred)\n",
    "            masked_sq_diff = sq_diff * mask\n",
    "            loss = tf.reduce_sum(masked_sq_diff) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "def masked_mse_intersection_loss(y_true, y_pred):\n",
    "    y_pred_img, x_ref = y_pred  # unpack model outputs\n",
    "    x_ren = y_true  # the true rendered image\n",
    "\n",
    "    epsilon = 1e-2\n",
    "    mask = tf.abs(x_ren - x_ref) > epsilon\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    sq_diff = tf.square(x_ren - y_pred_img)\n",
    "    masked_sq_diff = sq_diff * mask\n",
    "    denom = tf.reduce_sum(mask) + 1e-8\n",
    "\n",
    "    return tf.reduce_sum(masked_sq_diff) / denom\n",
    "\n",
    "\n",
    "def wrapper_loss(y_true, y_pred_outputs):\n",
    "    return masked_mse_intersection_loss(y_true, y_pred_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# def masked_mse_outside_intersection(x_ref, x_ren, epsilon=1e-2):\n",
    "#     def loss_fn(y_true, y_pred):\n",
    "#         # Create a mask: where rendered and reference are NOT approximately equal\n",
    "#         mask = tf.math.greater(tf.abs(x_ren - x_ref), epsilon)  # shape: (N, H, W, C)\n",
    "#         mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "#         # Compute squared difference\n",
    "#         sq_diff = tf.square(y_true - y_pred)\n",
    "\n",
    "#         # Apply mask\n",
    "#         masked_sq_diff = sq_diff * mask\n",
    "\n",
    "#         # Avoid dividing by zero\n",
    "#         denom = tf.reduce_sum(mask) + 1e-8\n",
    "\n",
    "#         return tf.reduce_sum(masked_sq_diff) / denom  # Mean over masked elements\n",
    "#     return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "model = CustomModel(inputs=base_model.inputs, outputs=base_model.output)\n",
    "\n",
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "# model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'])\n",
    "# Uncomment the following line to use a standard MSE loss instead\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_50_epochs', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [x_ref[:1025], eta_exp[:1025]],\n",
    "    x_ren[:1025],\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "model.save('models/k3_50epch_custom_loss_w_fit().h5')\n",
    "# early stopping disable for structure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4e37b",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])['prediction']\n",
    "\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5db5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])['prediction']\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33a1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28712fa6",
   "metadata": {},
   "source": [
    "## model (without custom loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c16efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras import layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47204e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs_wo_custom_loss', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        preds = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_100_epochs_wo_custom_loss', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [x_ref[:1025], eta_exp[:1025]],\n",
    "    x_ren[:1025],\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "        \n",
    "# early stopping disable for structure 4\n",
    "\n",
    "model.save('models/k3_100epch_wo_custom_loss_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fec169",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce68f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56669250",
   "metadata": {},
   "source": [
    "## custom loss model (without model.fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs_custom_loop_loss', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        preds = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c342bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(x_ref, x_ren, y_pred, epsilon=1e-2):\n",
    "    # # Create a mask where x_ref and x_ren are NOT approximately equal\n",
    "    # mask = tf.abs(x_ren - x_ref) > epsilon\n",
    "    # mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # # Compute squared error and apply the mask\n",
    "    # squared_diff = tf.square(y_pred - x_ren)\n",
    "    # masked_squared_diff = squared_diff * mask\n",
    "    \n",
    "    # Create a mask where x_ref and x_ren are approximately equal\n",
    "    mask = tf.math.less_equal(tf.abs(x_ren - x_ref), epsilon)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Overlay: only keep intersecting pixels in prediction by substituting with x_ref\n",
    "    overlay_preds = y_pred * (1 - mask) + x_ref * mask  # Same as np.where(mask, x_ref, y_pred)\n",
    "\n",
    "    # Compute MSE between x_ren and overlay_preds\n",
    "    mse = tf.reduce_mean(tf.square(x_ren - overlay_preds))\n",
    "\n",
    "    return mse\n",
    "\n",
    "    # Mean over only masked values\n",
    "    # loss = tf.reduce_sum(masked_squared_diff) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    # return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "split_idx = int(1025 * 0.9)\n",
    "\n",
    "x_ref_train = x_ref[:split_idx]\n",
    "eta_exp_train = eta_exp[:split_idx]\n",
    "x_ren_train = x_ren[:split_idx]\n",
    "\n",
    "x_ref_val = x_ref[split_idx:1025]\n",
    "eta_exp_val = eta_exp[split_idx:1025]\n",
    "x_ren_val = x_ren[split_idx:1025]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_ref_train, eta_exp_train, x_ren_train))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_ref_val, eta_exp_val, x_ren_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_50_epochs_custom_loop_loss', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "# Custom training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "\n",
    "    for x_ref_batch, eta_exp_batch, x_ren_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model([x_ref_batch, eta_exp_batch], training=True)\n",
    "            loss = masked_mse_loss(x_ref_batch, x_ren_batch, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss.update_state(loss)\n",
    "\n",
    "    # Validation loop\n",
    "    for x_ref_batch, eta_exp_batch, x_ren_batch in val_dataset:\n",
    "        y_pred = model([x_ref_batch, eta_exp_batch], training=False)\n",
    "        loss = masked_mse_loss(x_ref_batch, x_ren_batch, y_pred)\n",
    "        val_loss.update_state(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.result():.6f} - Val Loss: {val_loss.result():.6f}\")\n",
    "\n",
    "    callback.model = model  # Set the model for the callback\n",
    "    # Call visualization callback\n",
    "    callback.on_epoch_end(epoch)\n",
    "\n",
    "model.save('models/k1_50epch_custom_loop_loss_model.h5')\n",
    "# history = model.fit(\n",
    "#     [x_ref[:1025], eta_exp[:1025]],\n",
    "#     x_ren[:1025],\n",
    "#     epochs=50,\n",
    "#     batch_size=16,\n",
    "#     validation_split=0.1,\n",
    "#     callbacks=[callback]\n",
    "# )\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "        \n",
    "# early stopping disable for structure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d366",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140dff1",
   "metadata": {},
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras import layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ce55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Generator\n",
    "def build_generator(input_shape):\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # (H,W,6)\n",
    "\n",
    "    # Your chosen structure (structure 3)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    out_img = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "    return Model([x_ref_input, eta_exp_input], out_img, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(input_shape):\n",
    "    cond_input1 = Input(shape=input_shape, name='x_ref')\n",
    "    cond_input2 = Input(shape=input_shape, name='eta_exp')\n",
    "    img_input = Input(shape=input_shape, name='target_or_generated')\n",
    "\n",
    "    # Combine condition and image\n",
    "    x = Concatenate()([cond_input1, cond_input2, img_input])  # (H,W,9)\n",
    "    x = Conv2D(64, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(1, (4, 4), padding='same', activation='sigmoid')(x)  # PatchGAN output\n",
    "\n",
    "    return Model([cond_input1, cond_input2, img_input], x, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "input_shape = (500, 500, 3)\n",
    "generator = build_generator(input_shape)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile Discriminator separately\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combined GAN model (for training Generator)\n",
    "discriminator.trainable = False\n",
    "gen_output = generator([generator.inputs[0], generator.inputs[1]])\n",
    "validity = discriminator([generator.inputs[0], generator.inputs[1], gen_output])\n",
    "gan = Model(generator.inputs, [gen_output, validity])\n",
    "\n",
    "# Loss weights: L1 + adversarial\n",
    "lambda_l1 = 100\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred)) * lambda_l1\n",
    "\n",
    "gan.compile(\n",
    "    loss=[combined_loss, 'binary_crossentropy'],\n",
    "    loss_weights=[1, 1],\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "\n",
    "# Train\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "valid = np.ones((batch_size, 63, 63, 1))  # Patch size will depend on image size\n",
    "fake = np.zeros((batch_size, 63, 63, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(x_ref) // batch_size):\n",
    "        idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "        ref_batch = x_ref[idx]\n",
    "        tex_batch = eta_exp[idx]\n",
    "        real_batch = x_ren[idx]\n",
    "\n",
    "        # Generate images\n",
    "        gen_imgs = generator.predict([ref_batch, tex_batch])\n",
    "\n",
    "        # Train Discriminator\n",
    "        # d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch, \"target_or_generated\": real_batch},\n",
    "            valid\n",
    "        )\n",
    "\n",
    "        # d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "        d_loss_fake = discriminator.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch, \"target_or_generated\": gen_imgs},\n",
    "            fake\n",
    "        )\n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        # g_loss = gan.train_on_batch([ref_batch, tex_batch], [real_batch, valid])\n",
    "        g_loss = gan.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch},\n",
    "            [real_batch, valid]\n",
    "        )\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs}  D loss: {d_loss[0]:.4f}, acc: {d_loss[1]*100:.2f}% | G loss: {g_loss}\")\n",
    "\n",
    "# Save models\n",
    "generator.save('models/gan_generator.h5')\n",
    "discriminator.save('models/gan_discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Generator\n",
    "# ---------------------------\n",
    "def build_generator(input_shape):\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # (H,W,6)\n",
    "\n",
    "    x = Conv2D(32,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32,  (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    out_img = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='gen_out')(x)\n",
    "    return Model([x_ref_input, eta_exp_input], out_img, name=\"Generator\")\n",
    "\n",
    "# ---------------------------\n",
    "# Discriminator (PatchGAN)\n",
    "# ---------------------------\n",
    "def build_discriminator(input_shape):\n",
    "    x_ref_in = Input(shape=input_shape, name='x_ref')\n",
    "    eta_in   = Input(shape=input_shape, name='eta_exp')\n",
    "    img_in   = Input(shape=input_shape, name='target_or_generated')\n",
    "\n",
    "    x = Concatenate()([x_ref_in, eta_in, img_in])  # (H,W,9)\n",
    "    x = Conv2D(64,  (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1,   (4, 4), padding='same', activation='sigmoid', name='disc_out')(x)\n",
    "    return Model([x_ref_in, eta_in, img_in], out, name=\"Discriminator\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build\n",
    "# ---------------------------\n",
    "input_shape = (500, 500, 3)\n",
    "G = build_generator(input_shape)\n",
    "D = build_discriminator(input_shape)\n",
    "\n",
    "# Discriminator compile (no extra metrics to avoid update_state quirks)\n",
    "D.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "# Combined model for training G\n",
    "D.trainable = False\n",
    "gen_img = G({'x_ref': G.inputs[0], 'eta_exp': G.inputs[1]})\n",
    "validity = D({'x_ref': G.inputs[0], 'eta_exp': G.inputs[1], 'target_or_generated': gen_img})\n",
    "\n",
    "lambda_l1 = 100.0\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred)) * lambda_l1\n",
    "\n",
    "GAN = Model(G.inputs, [gen_img, validity], name='GAN')\n",
    "GAN.compile(\n",
    "    loss=[l1_loss, 'binary_crossentropy'],\n",
    "    loss_weights=[1.0, 1.0],\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop (expects x_ref, eta_exp, x_ren already loaded as float32 in [0,1])\n",
    "# ---------------------------\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "num_batches = x_ref.shape[0] // batch_size\n",
    "\n",
    "# Compute patch label shape dynamically\n",
    "tmp_y = D({\n",
    "    'x_ref': x_ref[:batch_size],\n",
    "    'eta_exp': eta_exp[:batch_size],\n",
    "    'target_or_generated': x_ren[:batch_size]\n",
    "}, training=False)\n",
    "patch_shape = tmp_y.shape[1:]             # e.g., (63, 63, 1)\n",
    "valid = np.ones((batch_size,) + tuple(patch_shape), dtype=np.float32)\n",
    "fake  = np.zeros((batch_size,) + tuple(patch_shape), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(num_batches):\n",
    "        idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "        ref_b = x_ref[idx]\n",
    "        tex_b = eta_exp[idx]\n",
    "        real_b = x_ren[idx]\n",
    "\n",
    "        # -----------------------\n",
    "        # Train D\n",
    "        # -----------------------\n",
    "        # Generate fakes (forward pass only)\n",
    "        gen_b = G({'x_ref': ref_b, 'eta_exp': tex_b}, training=True)\n",
    "\n",
    "        D.trainable = True\n",
    "        d_loss_real = D.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b, 'target_or_generated': real_b},\n",
    "            valid\n",
    "        )\n",
    "        d_loss_fake = D.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b, 'target_or_generated': gen_b},\n",
    "            fake\n",
    "        )\n",
    "        # d_loss may be a scalar or [loss] depending on Keras version\n",
    "        d_loss = 0.5 * (float(np.atleast_1d(d_loss_real)[0]) + float(np.atleast_1d(d_loss_fake)[0]))\n",
    "\n",
    "        # -----------------------\n",
    "        # Train G (adversarial + L1)\n",
    "        # -----------------------\n",
    "        D.trainable = False\n",
    "        g_losses = GAN.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b},\n",
    "            [real_b, valid]\n",
    "        )\n",
    "        # g_losses is [total, l1, adv] or similar depending on version; show total\n",
    "        g_loss_total = float(np.atleast_1d(g_losses)[0])\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs}  D loss: {d_loss:.4f} | G loss: {g_loss_total:.4f}\")\n",
    "\n",
    "# Save\n",
    "G.save('models/gan_generator.h5')\n",
    "D.save('models/gan_discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaeab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    inp = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(3, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\")(x)\n",
    "    return Model(inp, x, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    inp = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(1, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    return Model(inp, x, name=\"Discriminator\")\n",
    "\n",
    "# Optimizers\n",
    "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=disc_optimizer, metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# GAN model (discriminator not trainable in combined model)\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(64, 64, 3))\n",
    "fake_img = generator(gan_input)\n",
    "gan_output = discriminator(fake_img)\n",
    "gan = Model(gan_input, gan_output, name=\"GAN\")\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
    "\n",
    "print(generator.summary())\n",
    "print(discriminator.summary())\n",
    "print(gan.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6786ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Dense, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------\n",
    "#  Generator\n",
    "# -------------------\n",
    "def build_generator():\n",
    "    inp = Input(shape=(100,))  # random noise vector\n",
    "    x = Dense(8*8*128)(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Reshape((8, 8, 128))(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(3, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\")(x)\n",
    "    return Model(inp, x, name=\"Generator\")\n",
    "\n",
    "# -------------------\n",
    "#  Discriminator\n",
    "# -------------------\n",
    "def build_discriminator():\n",
    "    inp = Input(shape=(32, 32, 3))  # use 32x32 if that's your dataset size\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inp, x, name=\"Discriminator\")\n",
    "\n",
    "# -------------------\n",
    "#  Setup\n",
    "# -------------------\n",
    "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=disc_optimizer,\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "validity = discriminator(img)\n",
    "gan = Model(z, validity)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
    "\n",
    "# -------------------\n",
    "#  Training Loop\n",
    "# -------------------\n",
    "def train_gan(epochs=10000, batch_size=64, save_interval=1000):\n",
    "    # Load your dataset here (example: random noise as placeholder)\n",
    "    # Replace this with your real dataset (normalized to [-1,1])\n",
    "    (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "    X_train = X_train.astype(np.float32) / 127.5 - 1.0  # scale to [-1,1]\n",
    "    X_train = X_train.reshape((-1, 32, 32, 3))\n",
    "\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_imgs = generator.predict(noise, verbose=0)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_y = np.ones((batch_size, 1))  # want discriminator to think fakes are real\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "        # Save generated images\n",
    "        if epoch % save_interval == 0:\n",
    "            save_images(epoch, generator)\n",
    "\n",
    "# -------------------\n",
    "#  Save Images\n",
    "# -------------------\n",
    "def save_images(epoch, generator, examples=16):\n",
    "    noise = np.random.normal(0, 1, (examples, 100))\n",
    "    gen_imgs = generator.predict(noise, verbose=0)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # rescale [0,1]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(6,6))\n",
    "    cnt = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    os.makedirs(\"gan_images\", exist_ok=True)\n",
    "    fig.savefig(f\"gan_images/{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# -------------------\n",
    "#  Run Training\n",
    "# -------------------\n",
    "train_gan(epochs=5000, batch_size=64, save_interval=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7169bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model, layers, optimizers, preprocessing\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "assert x_ref.shape == eta_exp.shape == x_ren.shape, \"Dataset mismatch! Check folder sizes or images.\"\n",
    "\n",
    "# -----------------------\n",
    "# MODELS (dummy example)\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    out = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    out = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# -----------------------\n",
    "# COMBINED MODEL (GAN)\n",
    "# -----------------------\n",
    "ref_in = Input(shape=img_shape)\n",
    "tex_in = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_in, tex_in])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_in, tex_in, fake_ren])\n",
    "\n",
    "combined = Model([ref_in, tex_in], validity)\n",
    "combined.compile(optimizer=Adam(0.0002, 0.5), loss=\"binary_crossentropy\")\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "valid = np.ones((batch_size,) + (img_shape[0], img_shape[1], 1))\n",
    "fake = np.zeros((batch_size,) + (img_shape[0], img_shape[1], 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake rendered images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch])\n",
    "\n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator\n",
    "    g_loss = combined.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model, layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "\n",
    "# Generator\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape, name=\"ref_input\")\n",
    "    tex_in = Input(shape=img_shape, name=\"tex_input\")\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape, name=\"ref_input\")\n",
    "    tex_in = Input(shape=img_shape, name=\"tex_input\")\n",
    "    ren_in = Input(shape=img_shape, name=\"ren_input\")\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# Compile discriminator\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=disc_optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Combined GAN\n",
    "ref_input = Input(shape=img_shape, name=\"ref_input\")\n",
    "tex_input = Input(shape=img_shape, name=\"tex_input\")\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "# Match discriminator output shape\n",
    "valid = np.ones((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Random batch selection\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train Discriminator (use list, not dict!)\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator via GAN (use list)\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# OPTIONAL: Save models\n",
    "# -----------------------\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1351ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "#  DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 255.0  # normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500,500,3)\n",
    "\n",
    "# -----------------------\n",
    "#  GENERATOR\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], x, name=\"Generator\")\n",
    "\n",
    "# -----------------------\n",
    "#  DISCRIMINATOR\n",
    "# -----------------------\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)  # scalar output\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# -----------------------\n",
    "#  BUILD MODELS\n",
    "# -----------------------\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "gen_optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=disc_optimizer, metrics=['accuracy'])\n",
    "\n",
    "# GAN\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "# -----------------------\n",
    "#  TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "fake  = np.zeros((batch_size,1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Random batch\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator via GAN\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "#  SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938304b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # normalize [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500,500,3)\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# -----------------------\n",
    "# PATCHGAN DISCRIMINATOR\n",
    "# -----------------------\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])  # shape (500,500,9)\n",
    "\n",
    "    # PatchGAN layers\n",
    "    x = Conv2D(64, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(1, (4,4), padding='same', activation='sigmoid')(x)  # Patch output\n",
    "    return Model([ref_in, tex_in, ren_in], x, name=\"Discriminator\")\n",
    "\n",
    "# -----------------------\n",
    "# BUILD MODELS\n",
    "# -----------------------\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combined GAN\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "# PatchGAN output shape\n",
    "patch_shape = discriminator.output_shape[1:]  # e.g., (63,63,1)\n",
    "valid = np.ones((batch_size, *patch_shape), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, *patch_shape), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train Discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator via GAN\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fa4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 21:07:43.309748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 21:07:43.319820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755806863.327754   67466 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755806863.330196   67466 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-21 21:07:43.340085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755806879.342044   67466 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18146 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/lunet/bz0192/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n",
      "I0000 00:00:1755806879.912168   67844 service.cc:148] XLA service 0x77a3dc004000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755806879.912190   67844 service.cc:156]   StreamExecutor device (0): NVIDIA RTX 4000 Ada Generation, Compute Capability 8.9\n",
      "2025-08-21 21:08:00.003112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1755806880.073571   67844 cuda_dnn.cc:529] Loaded cuDNN version 90101\n",
      "I0000 00:00:1755806880.843020   67844 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "\n",
    "# Generator\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# -----------------------\n",
    "# COMPILE DISCRIMINATOR\n",
    "# -----------------------\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=disc_optimizer)\n",
    "\n",
    "# -----------------------\n",
    "# COMBINE INTO GAN\n",
    "# -----------------------\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "# Adjust discriminator target to output shape\n",
    "valid = np.ones((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # --- Train Discriminator ---\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # --- Train Generator ---\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    #print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4a0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling BatchNormalization.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: \u001b[0m\n\nArguments received by BatchNormalization.call():\n  • inputs=tf.Tensor(shape=(16, 500, 500, 128), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhaustedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Pick a small seed batch for visualization\u001b[39;00m\n\u001b[32m    178\u001b[39m seed_cond = cond_input[:\u001b[32m16\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m generate_and_save_images(generator, seed_cond, epoch+\u001b[32m1\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mgenerate_and_save_images\u001b[39m\u001b[34m(model, cond_input, epoch)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_and_save_images\u001b[39m(model, cond_input, epoch):\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     generated_images = model(cond_input, training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    155\u001b[39m     fig = plt.figure(figsize=(\u001b[32m4\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[32m16\u001b[39m, generated_images.shape[\u001b[32m0\u001b[39m])):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6000\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6001\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6002\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResourceExhaustedError\u001b[39m: Exception encountered when calling BatchNormalization.call().\n\n\u001b1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: \u001b[0m\n\nArguments received by BatchNormalization.call():\n  • inputs=tf.Tensor(shape=(16, 500, 500, 128), dtype=float32)\n  • training=False\n  • mask=None"
    ],
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython import display\n",
    "\n",
    "# -----------------------\n",
    "# LOAD DATA\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 127.5 - 1.0  # Normalize to [-1,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# CREATE DATASET\n",
    "# -----------------------\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = x_ref.shape[0]\n",
    "\n",
    "# Combine ref + texture as conditional input\n",
    "cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((cond_input, x_ren)) \\\n",
    "                               .shuffle(BUFFER_SIZE) \\\n",
    "                               .batch(BATCH_SIZE)\n",
    "\n",
    "# cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "# def data_generator(x, y, batch_size):\n",
    "#     idx = np.arange(len(x))\n",
    "#     while True:\n",
    "#         np.random.shuffle(idx)\n",
    "#         for i in range(0, len(x), batch_size):\n",
    "#             batch_idx = idx[i:i+batch_size]\n",
    "#             yield x[batch_idx], y[batch_idx]\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_generator(\n",
    "#     lambda: data_generator(cond_input, x_ren, BATCH_SIZE),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(None, 500, 500, 6), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(None, 500, 500, 3), dtype=tf.float32),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 6)))  # conditional input: ref + texture\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))  # output: RGB\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "# -----------------------\n",
    "# DISCRIMINATOR\n",
    "# -----------------------\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 9)))  # ref + texture + rendered\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# -----------------------\n",
    "# LOSS & OPTIMIZERS\n",
    "# -----------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN STEP\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def train_step(cond_images, real_images):\n",
    "    # cond_images: concatenated reference + texture (B, 500,500,6)\n",
    "    # real_images: rendered images (B,500,500,3)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(cond_images, training=True)\n",
    "\n",
    "        # Discriminator sees concatenated cond + real/fake\n",
    "        real_input = tf.concat([cond_images, real_images], axis=-1)\n",
    "        fake_input = tf.concat([cond_images, generated_images], axis=-1)\n",
    "\n",
    "        real_output = discriminator(real_input, training=True)\n",
    "        fake_output = discriminator(fake_input, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -----------------------\n",
    "# GENERATE & SAVE\n",
    "# -----------------------\n",
    "def generate_and_save_images(model, cond_input, epoch):\n",
    "    generated_images = model(cond_input, training=False)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(min(16, generated_images.shape[0])):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        img = (generated_images[i] + 1.0) / 2.0  # [-1,1] -> [0,1]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('gan/100epch/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for cond_batch, real_batch in train_dataset:\n",
    "        g_loss, d_loss = train_step(cond_batch, real_batch)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    # Pick a small seed batch for visualization\n",
    "    seed_cond = cond_input[:16]\n",
    "    generate_and_save_images(generator, seed_cond, epoch+1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, G loss: {g_loss:.4f}, D loss: {d_loss:.4f}, time: {time.time()-start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.matmul(a, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18ef879",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x73521fe89580> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython import display\n",
    "\n",
    "# -----------------------\n",
    "# LOAD DATA\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 127.5 - 1.0  # Normalize to [-1,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# CREATE DATASET\n",
    "# -----------------------\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = x_ref.shape[0]\n",
    "\n",
    "# Combine ref + texture as conditional input\n",
    "cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((cond_input, x_ren)) \\\n",
    "                               .shuffle(BUFFER_SIZE) \\\n",
    "                               .batch(BATCH_SIZE)\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 6)))  # conditional input: ref + texture\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))  # output: RGB\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "# -----------------------\n",
    "# DISCRIMINATOR\n",
    "# -----------------------\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 9)))  # ref + texture + rendered\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# -----------------------\n",
    "# LOSS & OPTIMIZERS\n",
    "# -----------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN STEP\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def train_step(cond_images, real_images):\n",
    "    # cond_images: concatenated reference + texture (B, 500,500,6)\n",
    "    # real_images: rendered images (B,500,500,3)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(cond_images, training=True)\n",
    "\n",
    "        # Cast generated images to float32\n",
    "        generated_images = tf.cast(generated_images, tf.float32)\n",
    "        \n",
    "        # Discriminator sees concatenated cond + real/fake\n",
    "        real_input = tf.concat([cond_images, real_images], axis=-1)\n",
    "        fake_input = tf.concat([cond_images, generated_images], axis=-1)\n",
    "\n",
    "        real_output = discriminator(real_input, training=True)\n",
    "        fake_output = discriminator(fake_input, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -----------------------\n",
    "# GENERATE & SAVE\n",
    "# -----------------------\n",
    "def generate_and_save_images(model, cond_input, epoch):\n",
    "    generated_images = model(cond_input, training=False)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(min(16, generated_images.shape[0])):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        img = (generated_images[i] + 1.0) / 2.0  # [-1,1] -> [0,1]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('gan/100epch/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for cond_batch, real_batch in train_dataset:\n",
    "        g_loss, d_loss = train_step(cond_batch, real_batch)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    # Pick a small seed batch for visualization\n",
    "    seed_cond = cond_input[:16]\n",
    "    generate_and_save_images(generator, seed_cond, epoch+1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, G loss: {g_loss:.4f}, D loss: {d_loss:.4f}, time: {time.time()-start:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
