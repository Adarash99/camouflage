{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f6fed6",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow numpy matplotlib torch torchvision pandas scikit-learn opencv-python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4d98e",
   "metadata": {},
   "source": [
    "## custom loss model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6841effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f120025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    model = Model(\n",
    "        inputs=[x_ref_input, eta_exp_input],\n",
    "        outputs={\"prediction\": x, \"x_ref_passthrough\": x_ref_input}\n",
    "    )\n",
    "\n",
    "    # Return both the prediction and the input x_ref (as passthrough)\n",
    "    #model = Model(inputs=[x_ref_input, eta_exp_input], outputs=[x, x_ref_input])\n",
    "\n",
    "    # model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        output_dict = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "        preds = output_dict[\"prediction\"]\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def masked_mse_only_on_diff(y_true, y_pred):\n",
    "    # `y_pred` is just the output from 'prediction'\n",
    "    # we will pull `x_ref` from the model's second output via `add_loss`\n",
    "\n",
    "    # Use Keras backend to store global reference\n",
    "    x_ref_tensor = tf.keras.backend.get_value(tf.keras.backend.learning_phase())  # placeholder\n",
    "    raise NotImplementedError(\"You can't access the other output here directly\")\n",
    "\n",
    "# This is just to show that Keras doesn't allow cross-output access in a pure loss function\n",
    "# So we use a custom training step instead — see next\n",
    "\n",
    "\n",
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        (x_ref, eta_exp), y_true = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self([x_ref, eta_exp], training=True)\n",
    "            y_pred = outputs[\"prediction\"]\n",
    "            x_ref_passthrough = outputs[\"x_ref_passthrough\"]\n",
    "\n",
    "            # Calculate intersection mask\n",
    "            epsilon = 1e-2\n",
    "            mask = tf.cast(tf.abs(y_true - x_ref_passthrough) > epsilon, tf.float32)\n",
    "            sq_diff = tf.square(y_true - y_pred)\n",
    "            masked_sq_diff = sq_diff * mask\n",
    "            loss = tf.reduce_sum(masked_sq_diff) / (tf.reduce_sum(mask) + 1e-8)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "def masked_mse_intersection_loss(y_true, y_pred):\n",
    "    y_pred_img, x_ref = y_pred  # unpack model outputs\n",
    "    x_ren = y_true  # the true rendered image\n",
    "\n",
    "    epsilon = 1e-2\n",
    "    mask = tf.abs(x_ren - x_ref) > epsilon\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    sq_diff = tf.square(x_ren - y_pred_img)\n",
    "    masked_sq_diff = sq_diff * mask\n",
    "    denom = tf.reduce_sum(mask) + 1e-8\n",
    "\n",
    "    return tf.reduce_sum(masked_sq_diff) / denom\n",
    "\n",
    "\n",
    "def wrapper_loss(y_true, y_pred_outputs):\n",
    "    return masked_mse_intersection_loss(y_true, y_pred_outputs)\n",
    "\n",
    "\n",
    "\n",
    "# def masked_mse_outside_intersection(x_ref, x_ren, epsilon=1e-2):\n",
    "#     def loss_fn(y_true, y_pred):\n",
    "#         # Create a mask: where rendered and reference are NOT approximately equal\n",
    "#         mask = tf.math.greater(tf.abs(x_ren - x_ref), epsilon)  # shape: (N, H, W, C)\n",
    "#         mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "#         # Compute squared difference\n",
    "#         sq_diff = tf.square(y_true - y_pred)\n",
    "\n",
    "#         # Apply mask\n",
    "#         masked_sq_diff = sq_diff * mask\n",
    "\n",
    "#         # Avoid dividing by zero\n",
    "#         denom = tf.reduce_sum(mask) + 1e-8\n",
    "\n",
    "#         return tf.reduce_sum(masked_sq_diff) / denom  # Mean over masked elements\n",
    "#     return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32cc9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "model = CustomModel(inputs=base_model.inputs, outputs=base_model.output)\n",
    "\n",
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "# model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', metrics=['accuracy'])\n",
    "# Uncomment the following line to use a standard MSE loss instead\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_50_epochs', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [x_ref[:1025], eta_exp[:1025]],\n",
    "    x_ren[:1025],\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "model.save('models/k3_50epch_custom_loss_w_fit().h5')\n",
    "# early stopping disable for structure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4e37b",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bcf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])['prediction']\n",
    "\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5db5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])['prediction']\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33a1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28712fa6",
   "metadata": {},
   "source": [
    "## model (without custom loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c16efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras import layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47204e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6a97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs_wo_custom_loss', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        preds = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_100_epochs_wo_custom_loss', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    [x_ref[:1025], eta_exp[:1025]],\n",
    "    x_ren[:1025],\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "        \n",
    "# early stopping disable for structure 4\n",
    "\n",
    "model.save('models/k3_100epch_wo_custom_loss_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fec169",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce68f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56669250",
   "metadata": {},
   "source": [
    "## custom loss model (without model.fit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_dtn(input_shape):\n",
    "    # Inputs\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "\n",
    "    # Combine inputs along channel dimension\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # Shape: (H, W, 6)\n",
    "\n",
    "    # structure 1\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 2\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # structure 3\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "\n",
    "    # structure 4\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = MaxPooling2D(pool_size=2, padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "    # # Crop to match the input shape\n",
    "    # x = tf.keras.layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "\n",
    "    # structure 5\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    # x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # final layer \n",
    "    x = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='prediction')(x)  # Output 3 channels\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[x_ref_input, eta_exp_input], outputs=x)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dbdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a12afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class IntersectionOverlayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_ref_val, eta_exp_val, x_ren_val, save_dir='overlay_outputs_50_epochs_custom_loop_loss', interval=1):\n",
    "        super().__init__()\n",
    "        self.x_ref_val = x_ref_val\n",
    "        self.eta_exp_val = eta_exp_val\n",
    "        self.x_ren_val = x_ren_val\n",
    "        self.interval = interval\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.interval != 0:\n",
    "            return\n",
    "\n",
    "        preds = self.model.predict([self.x_ref_val, self.eta_exp_val])\n",
    "\n",
    "\n",
    "        # Compute intersection (where rendered and reference are nearly equal)\n",
    "        intersection_mask = np.isclose(self.x_ren_val, self.x_ref_val, atol=1e-2)\n",
    "\n",
    "        # Overlay: only keep intersecting pixels in prediction\n",
    "        overlay_preds = np.where(intersection_mask, self.x_ref_val, preds)\n",
    "        #overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "        # Visualize or save first few samples\n",
    "        num_samples = min(3, len(preds))\n",
    "        for i in range(num_samples):\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "            axes[0].imshow(self.x_ref_val[i])\n",
    "            axes[0].set_title(\"x_ref\")\n",
    "            axes[1].imshow(self.x_ren_val[i])\n",
    "            axes[1].set_title(\"x_ren\")\n",
    "            axes[2].imshow(preds[i])\n",
    "            axes[2].set_title(\"prediction\")\n",
    "            axes[3].imshow(overlay_preds[i])\n",
    "            axes[3].set_title(\"pred ∩ intersection\")\n",
    "            for ax in axes:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.save_dir, f\"epoch_{epoch+1}_sample_{i+1}.png\"))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c342bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mse_loss(x_ref, x_ren, y_pred, epsilon=1e-2):\n",
    "    # # Create a mask where x_ref and x_ren are NOT approximately equal\n",
    "    # mask = tf.abs(x_ren - x_ref) > epsilon\n",
    "    # mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # # Compute squared error and apply the mask\n",
    "    # squared_diff = tf.square(y_pred - x_ren)\n",
    "    # masked_squared_diff = squared_diff * mask\n",
    "    \n",
    "    # Create a mask where x_ref and x_ren are approximately equal\n",
    "    mask = tf.math.less_equal(tf.abs(x_ren - x_ref), epsilon)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    # Overlay: only keep intersecting pixels in prediction by substituting with x_ref\n",
    "    overlay_preds = y_pred * (1 - mask) + x_ref * mask  # Same as np.where(mask, x_ref, y_pred)\n",
    "\n",
    "    # Compute MSE between x_ren and overlay_preds\n",
    "    mse = tf.reduce_mean(tf.square(x_ren - overlay_preds))\n",
    "\n",
    "    return mse\n",
    "\n",
    "    # Mean over only masked values\n",
    "    # loss = tf.reduce_sum(masked_squared_diff) / (tf.reduce_sum(mask) + 1e-8)\n",
    "    # return mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f358d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_ref, eta_exp, and x_ren are loaded with shape (N, H, W, 3)\n",
    "model = build_simple_dtn(input_shape=(500, 500, 3))\n",
    "\n",
    "batch_size = 16\n",
    "epochs = 50\n",
    "\n",
    "split_idx = int(1025 * 0.9)\n",
    "\n",
    "x_ref_train = x_ref[:split_idx]\n",
    "eta_exp_train = eta_exp[:split_idx]\n",
    "x_ren_train = x_ren[:split_idx]\n",
    "\n",
    "x_ref_val = x_ref[split_idx:1025]\n",
    "eta_exp_val = eta_exp[split_idx:1025]\n",
    "x_ren_val = x_ren[split_idx:1025]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_ref_train, eta_exp_train, x_ren_train))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_ref_val, eta_exp_val, x_ren_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# Use a small validation set to visualize overlays\n",
    "callback = IntersectionOverlayCallback(\n",
    "    x_ref_val=x_ref[1025:], \n",
    "    eta_exp_val=eta_exp[1025:], \n",
    "    x_ren_val=x_ren[1025:], \n",
    "    save_dir='overlay_outputs_50_epochs_custom_loop_loss', \n",
    "    interval=1  # every epoch\n",
    ")\n",
    "\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "# Custom training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "\n",
    "    for x_ref_batch, eta_exp_batch, x_ren_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model([x_ref_batch, eta_exp_batch], training=True)\n",
    "            loss = masked_mse_loss(x_ref_batch, x_ren_batch, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        train_loss.update_state(loss)\n",
    "\n",
    "    # Validation loop\n",
    "    for x_ref_batch, eta_exp_batch, x_ren_batch in val_dataset:\n",
    "        y_pred = model([x_ref_batch, eta_exp_batch], training=False)\n",
    "        loss = masked_mse_loss(x_ref_batch, x_ren_batch, y_pred)\n",
    "        val_loss.update_state(loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.result():.6f} - Val Loss: {val_loss.result():.6f}\")\n",
    "\n",
    "    callback.model = model  # Set the model for the callback\n",
    "    # Call visualization callback\n",
    "    callback.on_epoch_end(epoch)\n",
    "\n",
    "model.save('models/k1_50epch_custom_loop_loss_model.h5')\n",
    "# history = model.fit(\n",
    "#     [x_ref[:1025], eta_exp[:1025]],\n",
    "#     x_ren[:1025],\n",
    "#     epochs=50,\n",
    "#     batch_size=16,\n",
    "#     validation_split=0.1,\n",
    "#     callbacks=[callback]\n",
    "# )\n",
    "            # callbacks=[ \n",
    "            #     tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "            # ]\n",
    "        \n",
    "# early stopping disable for structure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00d366",
   "metadata": {},
   "source": [
    "#### Validation on val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading validation datasets...\")\n",
    "val_x_ref = load_images_from_folder('validation_dataset_clean/reference')\n",
    "val_eta_exp = load_images_from_folder('validation_dataset_clean/texture')\n",
    "val_x_ren = load_images_from_folder('validation_dataset_clean/rendered')\n",
    "print(val_x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = model.predict([val_x_ref, val_eta_exp])\n",
    "# Compute intersection (where rendered and reference are nearly equal)\n",
    "intersection_mask = np.isclose(val_x_ren, val_x_ref, atol=1e-2)\n",
    "\n",
    "# Overlay: only keep intersecting pixels in prediction\n",
    "overlay_preds = np.where(intersection_mask, val_x_ref, preds)\n",
    "# overlay_preds = np.where(intersection_mask, preds, 0.0)\n",
    "\n",
    "avg_mse = sklearn.metrics.mean_squared_error(val_x_ren.ravel(), overlay_preds.ravel())\n",
    "\n",
    "\n",
    "print(\"Average MSE: \", avg_mse)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479fcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict\n",
    "preds = model.predict([x_ref[1025:], eta_exp[1025:]])\n",
    "\n",
    "intersection_mask = np.isclose(x_ren[1025:], x_ref[1025:], atol=1e-2)\n",
    "overlay_preds = np.where(intersection_mask, x_ref[1025:], preds)\n",
    "\n",
    "num_samples = 5\n",
    "titles = [\"x_ref\", \"eta_exp\", \"x_ren\", \"prediction\", \"overlay\", \"intersection\"]\n",
    "\n",
    "plt.figure(figsize=(14, num_samples * 3))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    images = [\n",
    "        x_ref[i+1025],\n",
    "        eta_exp[i+1025],\n",
    "        x_ren[i+1025],\n",
    "        preds[i],\n",
    "        overlay_preds[i],\n",
    "        intersection_mask[i].astype(float)  # Show mask as image\n",
    "    ]\n",
    "    for j in range(6):\n",
    "        plt.subplot(num_samples, 6, i * 6 + j + 1)\n",
    "        plt.imshow(np.clip(images[j], 0, 1), cmap='gray' if j == 5 else None)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(titles[j])\n",
    "\n",
    "plt.suptitle(f\"Tensorflow out (50 epochs)\\nAVG MSE: {avg_mse}\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9140dff1",
   "metadata": {},
   "source": [
    "## GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras import layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ce55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = tf.keras.preprocessing.image.load_img(path, target_size=image_size)\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "\n",
    "print(\"Loading real datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "print(x_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model, layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, Flatten, Dense, MaxPooling2D, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Generator\n",
    "def build_generator(input_shape):\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # (H,W,6)\n",
    "\n",
    "    # Your chosen structure (structure 3)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    out_img = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)\n",
    "    return Model([x_ref_input, eta_exp_input], out_img, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(input_shape):\n",
    "    cond_input1 = Input(shape=input_shape, name='x_ref')\n",
    "    cond_input2 = Input(shape=input_shape, name='eta_exp')\n",
    "    img_input = Input(shape=input_shape, name='target_or_generated')\n",
    "\n",
    "    # Combine condition and image\n",
    "    x = Concatenate()([cond_input1, cond_input2, img_input])  # (H,W,9)\n",
    "    x = Conv2D(64, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(1, (4, 4), padding='same', activation='sigmoid')(x)  # PatchGAN output\n",
    "\n",
    "    return Model([cond_input1, cond_input2, img_input], x, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "input_shape = (500, 500, 3)\n",
    "generator = build_generator(input_shape)\n",
    "discriminator = build_discriminator(input_shape)\n",
    "\n",
    "# Compile Discriminator separately\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combined GAN model (for training Generator)\n",
    "discriminator.trainable = False\n",
    "gen_output = generator([generator.inputs[0], generator.inputs[1]])\n",
    "validity = discriminator([generator.inputs[0], generator.inputs[1], gen_output])\n",
    "gan = Model(generator.inputs, [gen_output, validity])\n",
    "\n",
    "# Loss weights: L1 + adversarial\n",
    "lambda_l1 = 100\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred)) * lambda_l1\n",
    "\n",
    "gan.compile(\n",
    "    loss=[combined_loss, 'binary_crossentropy'],\n",
    "    loss_weights=[1, 1],\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "\n",
    "# Train\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "valid = np.ones((batch_size, 63, 63, 1))  # Patch size will depend on image size\n",
    "fake = np.zeros((batch_size, 63, 63, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(x_ref) // batch_size):\n",
    "        idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "        ref_batch = x_ref[idx]\n",
    "        tex_batch = eta_exp[idx]\n",
    "        real_batch = x_ren[idx]\n",
    "\n",
    "        # Generate images\n",
    "        gen_imgs = generator.predict([ref_batch, tex_batch])\n",
    "\n",
    "        # Train Discriminator\n",
    "        # d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch, \"target_or_generated\": real_batch},\n",
    "            valid\n",
    "        )\n",
    "\n",
    "        # d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "        d_loss_fake = discriminator.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch, \"target_or_generated\": gen_imgs},\n",
    "            fake\n",
    "        )\n",
    "\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # Train Generator\n",
    "        # g_loss = gan.train_on_batch([ref_batch, tex_batch], [real_batch, valid])\n",
    "        g_loss = gan.train_on_batch(\n",
    "            {\"x_ref\": ref_batch, \"eta_exp\": tex_batch},\n",
    "            [real_batch, valid]\n",
    "        )\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs}  D loss: {d_loss[0]:.4f}, acc: {d_loss[1]*100:.2f}% | G loss: {g_loss}\")\n",
    "\n",
    "# Save models\n",
    "generator.save('models/gan_generator.h5')\n",
    "discriminator.save('models/gan_discriminator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa2ab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Generator\n",
    "# ---------------------------\n",
    "def build_generator(input_shape):\n",
    "    x_ref_input = Input(shape=input_shape, name='x_ref')\n",
    "    eta_exp_input = Input(shape=input_shape, name='eta_exp')\n",
    "    x = Concatenate()([x_ref_input, eta_exp_input])  # (H,W,6)\n",
    "\n",
    "    x = Conv2D(32,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(64,  (3, 3), activation='relu', padding='same')(x)\n",
    "    x = Conv2D(32,  (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    out_img = Conv2D(3, (1, 1), activation='sigmoid', padding='same', name='gen_out')(x)\n",
    "    return Model([x_ref_input, eta_exp_input], out_img, name=\"Generator\")\n",
    "\n",
    "# ---------------------------\n",
    "# Discriminator (PatchGAN)\n",
    "# ---------------------------\n",
    "def build_discriminator(input_shape):\n",
    "    x_ref_in = Input(shape=input_shape, name='x_ref')\n",
    "    eta_in   = Input(shape=input_shape, name='eta_exp')\n",
    "    img_in   = Input(shape=input_shape, name='target_or_generated')\n",
    "\n",
    "    x = Concatenate()([x_ref_in, eta_in, img_in])  # (H,W,9)\n",
    "    x = Conv2D(64,  (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4, 4), strides=2, padding='same')(x);  x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1,   (4, 4), padding='same', activation='sigmoid', name='disc_out')(x)\n",
    "    return Model([x_ref_in, eta_in, img_in], out, name=\"Discriminator\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build\n",
    "# ---------------------------\n",
    "input_shape = (500, 500, 3)\n",
    "G = build_generator(input_shape)\n",
    "D = build_discriminator(input_shape)\n",
    "\n",
    "# Discriminator compile (no extra metrics to avoid update_state quirks)\n",
    "D.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "# Combined model for training G\n",
    "D.trainable = False\n",
    "gen_img = G({'x_ref': G.inputs[0], 'eta_exp': G.inputs[1]})\n",
    "validity = D({'x_ref': G.inputs[0], 'eta_exp': G.inputs[1], 'target_or_generated': gen_img})\n",
    "\n",
    "lambda_l1 = 100.0\n",
    "def l1_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred)) * lambda_l1\n",
    "\n",
    "GAN = Model(G.inputs, [gen_img, validity], name='GAN')\n",
    "GAN.compile(\n",
    "    loss=[l1_loss, 'binary_crossentropy'],\n",
    "    loss_weights=[1.0, 1.0],\n",
    "    optimizer=Adam(0.0002, 0.5)\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop (expects x_ref, eta_exp, x_ren already loaded as float32 in [0,1])\n",
    "# ---------------------------\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "num_batches = x_ref.shape[0] // batch_size\n",
    "\n",
    "# Compute patch label shape dynamically\n",
    "tmp_y = D({\n",
    "    'x_ref': x_ref[:batch_size],\n",
    "    'eta_exp': eta_exp[:batch_size],\n",
    "    'target_or_generated': x_ren[:batch_size]\n",
    "}, training=False)\n",
    "patch_shape = tmp_y.shape[1:]             # e.g., (63, 63, 1)\n",
    "valid = np.ones((batch_size,) + tuple(patch_shape), dtype=np.float32)\n",
    "fake  = np.zeros((batch_size,) + tuple(patch_shape), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(num_batches):\n",
    "        idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "        ref_b = x_ref[idx]\n",
    "        tex_b = eta_exp[idx]\n",
    "        real_b = x_ren[idx]\n",
    "\n",
    "        # -----------------------\n",
    "        # Train D\n",
    "        # -----------------------\n",
    "        # Generate fakes (forward pass only)\n",
    "        gen_b = G({'x_ref': ref_b, 'eta_exp': tex_b}, training=True)\n",
    "\n",
    "        D.trainable = True\n",
    "        d_loss_real = D.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b, 'target_or_generated': real_b},\n",
    "            valid\n",
    "        )\n",
    "        d_loss_fake = D.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b, 'target_or_generated': gen_b},\n",
    "            fake\n",
    "        )\n",
    "        # d_loss may be a scalar or [loss] depending on Keras version\n",
    "        d_loss = 0.5 * (float(np.atleast_1d(d_loss_real)[0]) + float(np.atleast_1d(d_loss_fake)[0]))\n",
    "\n",
    "        # -----------------------\n",
    "        # Train G (adversarial + L1)\n",
    "        # -----------------------\n",
    "        D.trainable = False\n",
    "        g_losses = GAN.train_on_batch(\n",
    "            {'x_ref': ref_b, 'eta_exp': tex_b},\n",
    "            [real_b, valid]\n",
    "        )\n",
    "        # g_losses is [total, l1, adv] or similar depending on version; show total\n",
    "        g_loss_total = float(np.atleast_1d(g_losses)[0])\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs}  D loss: {d_loss:.4f} | G loss: {g_loss_total:.4f}\")\n",
    "\n",
    "# Save\n",
    "G.save('models/gan_generator.h5')\n",
    "D.save('models/gan_discriminator.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaeab30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generator\n",
    "def build_generator():\n",
    "    inp = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(3, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\")(x)\n",
    "    return Model(inp, x, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    inp = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(1, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    return Model(inp, x, name=\"Discriminator\")\n",
    "\n",
    "# Optimizers\n",
    "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "# Build models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(\n",
    "    loss=\"binary_crossentropy\", optimizer=disc_optimizer, metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# GAN model (discriminator not trainable in combined model)\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(64, 64, 3))\n",
    "fake_img = generator(gan_input)\n",
    "gan_output = discriminator(fake_img)\n",
    "gan = Model(gan_input, gan_output, name=\"GAN\")\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
    "\n",
    "print(generator.summary())\n",
    "print(discriminator.summary())\n",
    "print(gan.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6786ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Dense, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------\n",
    "#  Generator\n",
    "# -------------------\n",
    "def build_generator():\n",
    "    inp = Input(shape=(100,))  # random noise vector\n",
    "    x = Dense(8*8*128)(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Reshape((8, 8, 128))(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(64, kernel_size=3, strides=1, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = tf.keras.layers.UpSampling2D()(x)\n",
    "    x = Conv2D(3, kernel_size=3, strides=1, padding=\"same\", activation=\"tanh\")(x)\n",
    "    return Model(inp, x, name=\"Generator\")\n",
    "\n",
    "# -------------------\n",
    "#  Discriminator\n",
    "# -------------------\n",
    "def build_discriminator():\n",
    "    inp = Input(shape=(32, 32, 3))  # use 32x32 if that's your dataset size\n",
    "    x = Conv2D(64, kernel_size=3, strides=2, padding=\"same\")(inp)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Conv2D(128, kernel_size=3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    return Model(inp, x, name=\"Discriminator\")\n",
    "\n",
    "# -------------------\n",
    "#  Setup\n",
    "# -------------------\n",
    "gen_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_optimizer = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "discriminator.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=disc_optimizer,\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "# GAN model\n",
    "discriminator.trainable = False\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "validity = discriminator(img)\n",
    "gan = Model(z, validity)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=gen_optimizer)\n",
    "\n",
    "# -------------------\n",
    "#  Training Loop\n",
    "# -------------------\n",
    "def train_gan(epochs=10000, batch_size=64, save_interval=1000):\n",
    "    # Load your dataset here (example: random noise as placeholder)\n",
    "    # Replace this with your real dataset (normalized to [-1,1])\n",
    "    (X_train, _), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
    "    X_train = X_train.astype(np.float32) / 127.5 - 1.0  # scale to [-1,1]\n",
    "    X_train = X_train.reshape((-1, 32, 32, 3))\n",
    "\n",
    "    half_batch = batch_size // 2\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        real_imgs = X_train[idx]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "        fake_imgs = generator.predict(noise, verbose=0)\n",
    "\n",
    "        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        valid_y = np.ones((batch_size, 1))  # want discriminator to think fakes are real\n",
    "        g_loss = gan.train_on_batch(noise, valid_y)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"{epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "        # Save generated images\n",
    "        if epoch % save_interval == 0:\n",
    "            save_images(epoch, generator)\n",
    "\n",
    "# -------------------\n",
    "#  Save Images\n",
    "# -------------------\n",
    "def save_images(epoch, generator, examples=16):\n",
    "    noise = np.random.normal(0, 1, (examples, 100))\n",
    "    gen_imgs = generator.predict(noise, verbose=0)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # rescale [0,1]\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(6,6))\n",
    "    cnt = 0\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axs[i,j].imshow(gen_imgs[cnt])\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    os.makedirs(\"gan_images\", exist_ok=True)\n",
    "    fig.savefig(f\"gan_images/{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "# -------------------\n",
    "#  Run Training\n",
    "# -------------------\n",
    "train_gan(epochs=5000, batch_size=64, save_interval=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7169bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model, layers, optimizers, preprocessing\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "assert x_ref.shape == eta_exp.shape == x_ren.shape, \"Dataset mismatch! Check folder sizes or images.\"\n",
    "\n",
    "# -----------------------\n",
    "# MODELS (dummy example)\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    out = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    out = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# Compile discriminator\n",
    "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# -----------------------\n",
    "# COMBINED MODEL (GAN)\n",
    "# -----------------------\n",
    "ref_in = Input(shape=img_shape)\n",
    "tex_in = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_in, tex_in])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_in, tex_in, fake_ren])\n",
    "\n",
    "combined = Model([ref_in, tex_in], validity)\n",
    "combined.compile(optimizer=Adam(0.0002, 0.5), loss=\"binary_crossentropy\")\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "valid = np.ones((batch_size,) + (img_shape[0], img_shape[1], 1))\n",
    "fake = np.zeros((batch_size,) + (img_shape[0], img_shape[1], 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake rendered images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch])\n",
    "\n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator\n",
    "    g_loss = combined.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model, layers, optimizers\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # Normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "\n",
    "# Generator\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape, name=\"ref_input\")\n",
    "    tex_in = Input(shape=img_shape, name=\"tex_input\")\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape, name=\"ref_input\")\n",
    "    tex_in = Input(shape=img_shape, name=\"tex_input\")\n",
    "    ren_in = Input(shape=img_shape, name=\"ren_input\")\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# Compile discriminator\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "discriminator.compile(loss=\"binary_crossentropy\", optimizer=disc_optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# Combined GAN\n",
    "ref_input = Input(shape=img_shape, name=\"ref_input\")\n",
    "tex_input = Input(shape=img_shape, name=\"tex_input\")\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "# Match discriminator output shape\n",
    "valid = np.ones((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Random batch selection\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train Discriminator (use list, not dict!)\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator via GAN (use list)\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# OPTIONAL: Save models\n",
    "# -----------------------\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1351ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "#  DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 255.0  # normalize to [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500,500,3)\n",
    "\n",
    "# -----------------------\n",
    "#  GENERATOR\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], x, name=\"Generator\")\n",
    "\n",
    "# -----------------------\n",
    "#  DISCRIMINATOR\n",
    "# -----------------------\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (3,3), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)  # scalar output\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# -----------------------\n",
    "#  BUILD MODELS\n",
    "# -----------------------\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "gen_optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=disc_optimizer, metrics=['accuracy'])\n",
    "\n",
    "# GAN\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "\n",
    "# -----------------------\n",
    "#  TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "valid = np.ones((batch_size,1), dtype=np.float32)\n",
    "fake  = np.zeros((batch_size,1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Random batch\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train generator via GAN\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "#  SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938304b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img)\n",
    "        img = img / 255.0  # normalize [0,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "img_shape = x_ref.shape[1:]  # (500,500,3)\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# -----------------------\n",
    "# PATCHGAN DISCRIMINATOR\n",
    "# -----------------------\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])  # shape (500,500,9)\n",
    "\n",
    "    # PatchGAN layers\n",
    "    x = Conv2D(64, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(128, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(256, (4,4), strides=2, padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Conv2D(1, (4,4), padding='same', activation='sigmoid')(x)  # Patch output\n",
    "    return Model([ref_in, tex_in, ren_in], x, name=\"Discriminator\")\n",
    "\n",
    "# -----------------------\n",
    "# BUILD MODELS\n",
    "# -----------------------\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.5), metrics=['accuracy'])\n",
    "\n",
    "# Combined GAN\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "# PatchGAN output shape\n",
    "patch_shape = discriminator.output_shape[1:]  # e.g., (63,63,1)\n",
    "valid = np.ones((batch_size, *patch_shape), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, *patch_shape), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # Train Discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator via GAN\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fa4c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 21:07:43.309748: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 21:07:43.319820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755806863.327754   67466 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755806863.330196   67466 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-21 21:07:43.340085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755806879.342044   67466 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18146 MB memory:  -> device: 0, name: NVIDIA RTX 4000 Ada Generation, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/lunet/bz0192/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_1']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n",
      "I0000 00:00:1755806879.912168   67844 service.cc:148] XLA service 0x77a3dc004000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755806879.912190   67844 service.cc:156]   StreamExecutor device (0): NVIDIA RTX 4000 Ada Generation, Compute Capability 8.9\n",
      "2025-08-21 21:08:00.003112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1755806880.073571   67844 cuda_dnn.cc:529] Loaded cuDNN version 90101\n",
      "I0000 00:00:1755806880.843020   67844 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'update_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 99\u001b[39m\n\u001b[32m     96\u001b[39m gen_imgs = generator.predict([ref_batch, tex_batch], verbose=\u001b[32m0\u001b[39m)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# --- Train Discriminator ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n\u001b[32m    100\u001b[39m d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n\u001b[32m    101\u001b[39m d_loss = \u001b[32m0.5\u001b[39m * np.add(d_loss_real, d_loss_fake)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:549\u001b[39m, in \u001b[36mTensorFlowTrainer.train_on_batch\u001b[39m\u001b[34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[39m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdata\u001b[39m():\n\u001b[32m    547\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m logs = \u001b[38;5;28mself\u001b[39m.train_function(data())\n\u001b[32m    550\u001b[39m logs = tree.map_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np.array(x), logs)\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:121\u001b[39m, in \u001b[36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m data = \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.distribute_strategy.run(\n\u001b[32m    122\u001b[39m     one_step_on_data, args=(data,)\n\u001b[32m    123\u001b[39m )\n\u001b[32m    124\u001b[39m outputs = reduce_per_replica(\n\u001b[32m    125\u001b[39m     outputs,\n\u001b[32m    126\u001b[39m     \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    127\u001b[39m     reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    128\u001b[39m )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:108\u001b[39m, in \u001b[36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_step(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:61\u001b[39m, in \u001b[36mTensorFlowTrainer.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     53\u001b[39m     y_pred = \u001b[38;5;28mself\u001b[39m(x)\n\u001b[32m     54\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._compute_loss(\n\u001b[32m     55\u001b[39m     x=x,\n\u001b[32m     56\u001b[39m     y=y,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     training=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     60\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28mself\u001b[39m._loss_tracker.update_state(\n\u001b[32m     62\u001b[39m     loss, sample_weight=tf.shape(tree.flatten(x)[\u001b[32m0\u001b[39m])[\u001b[32m0\u001b[39m]\n\u001b[32m     63\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     65\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.optimizer.scale_loss(loss)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'update_state'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from keras import Input, Model\n",
    "from keras.layers import Conv2D, Concatenate, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# DATA LOADING\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 255.0  # Normalize\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# MODEL DEFINITIONS\n",
    "# -----------------------\n",
    "img_shape = x_ref.shape[1:]  # (500, 500, 3)\n",
    "\n",
    "# Generator\n",
    "def build_generator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in], out, name=\"Generator\")\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator(img_shape):\n",
    "    ref_in = Input(shape=img_shape)\n",
    "    tex_in = Input(shape=img_shape)\n",
    "    ren_in = Input(shape=img_shape)\n",
    "    x = Concatenate()([ref_in, tex_in, ren_in])\n",
    "    x = Conv2D(64, (3,3), padding='same')(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    out = Conv2D(1, (3,3), activation='sigmoid', padding='same')(x)\n",
    "    return Model([ref_in, tex_in, ren_in], out, name=\"Discriminator\")\n",
    "\n",
    "# Build models\n",
    "generator = build_generator(img_shape)\n",
    "discriminator = build_discriminator(img_shape)\n",
    "\n",
    "# -----------------------\n",
    "# COMPILE DISCRIMINATOR\n",
    "# -----------------------\n",
    "disc_optimizer = Adam(0.0002, 0.5)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=disc_optimizer)\n",
    "\n",
    "# -----------------------\n",
    "# COMBINE INTO GAN\n",
    "# -----------------------\n",
    "ref_input = Input(shape=img_shape)\n",
    "tex_input = Input(shape=img_shape)\n",
    "fake_ren = generator([ref_input, tex_input])\n",
    "discriminator.trainable = False\n",
    "validity = discriminator([ref_input, tex_input, fake_ren])\n",
    "gan = Model([ref_input, tex_input], validity)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# -----------------------\n",
    "# TRAINING LOOP\n",
    "# -----------------------\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "\n",
    "# Adjust discriminator target to output shape\n",
    "valid = np.ones((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "fake = np.zeros((batch_size, img_shape[0], img_shape[1], 1), dtype=np.float32)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.randint(0, x_ref.shape[0], batch_size)\n",
    "    ref_batch = x_ref[idx]\n",
    "    tex_batch = eta_exp[idx]\n",
    "    real_batch = x_ren[idx]\n",
    "\n",
    "    # Generate fake images\n",
    "    gen_imgs = generator.predict([ref_batch, tex_batch], verbose=0)\n",
    "\n",
    "    # --- Train Discriminator ---\n",
    "    d_loss_real = discriminator.train_on_batch([ref_batch, tex_batch, real_batch], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([ref_batch, tex_batch, gen_imgs], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # --- Train Generator ---\n",
    "    g_loss = gan.train_on_batch([ref_batch, tex_batch], valid)\n",
    "\n",
    "    #print(f\"{epoch+1}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}] [G loss: {g_loss:.4f}]\")\n",
    "\n",
    "# -----------------------\n",
    "# SAVE MODELS\n",
    "# -----------------------\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "generator.save(\"models/gan_generator.h5\")\n",
    "discriminator.save(\"models/gan_discriminator.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f4a0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 155\u001b[39m\n\u001b[32m    152\u001b[39m start = time.time()\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cond_batch, real_batch \u001b[38;5;129;01min\u001b[39;00m train_dataset:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     g_loss, d_loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcond_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m display.clear_output(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Pick a small seed batch for visualization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    866\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    867\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    868\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    873\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    874\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    875\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\bz0192\\.conda\\envs\\PROJECT\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "2025-08-21 21:19:43.592210: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-08-21 21:19:43.592238: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-08-21 21:19:43.830423: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-08-21 21:19:43.941769: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-08-21 21:19:44.918721: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.93GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-08-21 21:20:03.904596: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:497] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.91GiB (rounded to 2048000000)requested by op AddV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-21 21:20:03.904638: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1053] BFCAllocator dump for GPU_0_bfc\n",
      "2025-08-21 21:20:03.904648: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (256): \tTotal Chunks: 88, Chunks in use: 78. 22.0KiB allocated for chunks. 19.5KiB in use in bin. 6.5KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904653: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (512): \tTotal Chunks: 17, Chunks in use: 16. 9.0KiB allocated for chunks. 8.5KiB in use in bin. 8.0KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904658: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1024): \tTotal Chunks: 2, Chunks in use: 1. 2.5KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904663: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904668: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904672: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 8.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904677: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16384): \tTotal Chunks: 3, Chunks in use: 2. 78.5KiB allocated for chunks. 49.8KiB in use in bin. 37.5KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904682: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 260.5KiB allocated for chunks. 260.5KiB in use in bin. 243.8KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904687: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (65536): \tTotal Chunks: 1, Chunks in use: 1. 74.8KiB allocated for chunks. 74.8KiB in use in bin. 56.2KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904691: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904695: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904700: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (524288): \tTotal Chunks: 9, Chunks in use: 8. 7.05MiB allocated for chunks. 6.30MiB in use in bin. 6.25MiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904704: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (1048576): \tTotal Chunks: 2, Chunks in use: 1. 2.61MiB allocated for chunks. 1.40MiB in use in bin. 800.0KiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904708: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904713: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (4194304): \tTotal Chunks: 4, Chunks in use: 3. 30.51MiB allocated for chunks. 22.89MiB in use in bin. 22.89MiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904717: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904722: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904726: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904730: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 1. 91.55MiB allocated for chunks. 91.55MiB in use in bin. 91.55MiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904740: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904744: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1060] Bin (268435456): \tTotal Chunks: 9, Chunks in use: 7. 17.59GiB allocated for chunks. 15.31GiB in use in bin. 15.31GiB client-requested in use in bin.\n",
      "2025-08-21 21:20:03.904749: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1076] Bin for 1.91GiB was 256.00MiB, Chunk State: \n",
      "2025-08-21 21:20:03.904758: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1082]   Size: 880.93MiB | Requested Size: 16.00MiB | in_use: 0 | bin_num: 20, prev:   Size: 91.55MiB | Requested Size: 91.55MiB | in_use: 1 | bin_num: -1, next:   Size: 7.63MiB | Requested Size: 7.63MiB | in_use: 1 | bin_num: -1\n",
      "2025-08-21 21:20:03.904763: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1082]   Size: 1.42GiB | Requested Size: 993.34MiB | in_use: 0 | bin_num: 20, prev:   Size: 1.91GiB | Requested Size: 1.91GiB | in_use: 1 | bin_num: -1\n",
      "2025-08-21 21:20:03.904767: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1089] Next region of size 19028312064\n",
      "2025-08-21 21:20:03.904773: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785c8e000000 of size 6180000000 next 1\n",
      "2025-08-21 21:20:03.904795: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785dfe5b5100 of size 1280 next 2\n",
      "2025-08-21 21:20:03.904799: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785dfe5b5600 of size 3090000128 next 3\n",
      "2025-08-21 21:20:03.904803: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb688ff00 of size 256 next 4\n",
      "2025-08-21 21:20:03.904807: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890000 of size 256 next 5\n",
      "2025-08-21 21:20:03.904810: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890100 of size 256 next 6\n",
      "2025-08-21 21:20:03.904814: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890200 of size 256 next 7\n",
      "2025-08-21 21:20:03.904818: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890300 of size 256 next 8\n",
      "2025-08-21 21:20:03.904822: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890400 of size 256 next 9\n",
      "2025-08-21 21:20:03.904825: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890500 of size 256 next 10\n",
      "2025-08-21 21:20:03.904829: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890600 of size 256 next 11\n",
      "2025-08-21 21:20:03.904833: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890700 of size 256 next 13\n",
      "2025-08-21 21:20:03.904836: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890800 of size 256 next 14\n",
      "2025-08-21 21:20:03.904840: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890900 of size 256 next 12\n",
      "2025-08-21 21:20:03.904844: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890a00 of size 256 next 15\n",
      "2025-08-21 21:20:03.904847: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890b00 of size 512 next 19\n",
      "2025-08-21 21:20:03.904851: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890d00 of size 256 next 20\n",
      "2025-08-21 21:20:03.904855: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890e00 of size 256 next 21\n",
      "2025-08-21 21:20:03.904858: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6890f00 of size 512 next 22\n",
      "2025-08-21 21:20:03.904862: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891100 of size 512 next 25\n",
      "2025-08-21 21:20:03.904866: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891300 of size 512 next 23\n",
      "2025-08-21 21:20:03.904870: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891500 of size 512 next 110\n",
      "2025-08-21 21:20:03.904874: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891700 of size 256 next 105\n",
      "2025-08-21 21:20:03.904878: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891800 of size 256 next 28\n",
      "2025-08-21 21:20:03.904881: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891900 of size 256 next 30\n",
      "2025-08-21 21:20:03.904885: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891a00 of size 256 next 31\n",
      "2025-08-21 21:20:03.904889: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891b00 of size 256 next 32\n",
      "2025-08-21 21:20:03.904892: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891c00 of size 256 next 29\n",
      "2025-08-21 21:20:03.904896: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891d00 of size 256 next 34\n",
      "2025-08-21 21:20:03.904900: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891e00 of size 256 next 35\n",
      "2025-08-21 21:20:03.904903: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6891f00 of size 256 next 36\n",
      "2025-08-21 21:20:03.904907: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892000 of size 256 next 37\n",
      "2025-08-21 21:20:03.904911: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892100 of size 256 next 39\n",
      "2025-08-21 21:20:03.904914: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892200 of size 256 next 40\n",
      "2025-08-21 21:20:03.904918: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892300 of size 256 next 38\n",
      "2025-08-21 21:20:03.904922: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb6892400 of size 256 next 44\n",
      "2025-08-21 21:20:03.904925: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892500 of size 512 next 43\n",
      "2025-08-21 21:20:03.904929: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892700 of size 256 next 47\n",
      "2025-08-21 21:20:03.904933: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892800 of size 256 next 41\n",
      "2025-08-21 21:20:03.904936: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892900 of size 256 next 50\n",
      "2025-08-21 21:20:03.904940: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892a00 of size 256 next 51\n",
      "2025-08-21 21:20:03.904944: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892b00 of size 256 next 54\n",
      "2025-08-21 21:20:03.904947: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6892c00 of size 36352 next 42\n",
      "2025-08-21 21:20:03.904951: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb689ba00 of size 31744 next 16\n",
      "2025-08-21 21:20:03.904955: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68a3600 of size 38400 next 17\n",
      "2025-08-21 21:20:03.904959: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68acc00 of size 38400 next 52\n",
      "2025-08-21 21:20:03.904963: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68b6200 of size 256 next 53\n",
      "2025-08-21 21:20:03.904966: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68b6300 of size 76544 next 46\n",
      "2025-08-21 21:20:03.904970: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68c8e00 of size 57600 next 45\n",
      "2025-08-21 21:20:03.904974: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68d6f00 of size 512 next 57\n",
      "2025-08-21 21:20:03.904978: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68d7100 of size 256 next 59\n",
      "2025-08-21 21:20:03.904982: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68d7200 of size 256 next 60\n",
      "2025-08-21 21:20:03.904986: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb68d7300 of size 1464576 next 27\n",
      "2025-08-21 21:20:03.904990: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6a3cc00 of size 819200 next 26\n",
      "2025-08-21 21:20:03.904993: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6b04c00 of size 819200 next 33\n",
      "2025-08-21 21:20:03.904997: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6bccc00 of size 819200 next 49\n",
      "2025-08-21 21:20:03.905001: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6c94c00 of size 819200 next 48\n",
      "2025-08-21 21:20:03.905005: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb6d5cc00 of size 8000000 next 58\n",
      "2025-08-21 21:20:03.905008: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fde00 of size 256 next 61\n",
      "2025-08-21 21:20:03.905012: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fdf00 of size 256 next 62\n",
      "2025-08-21 21:20:03.905016: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe000 of size 256 next 63\n",
      "2025-08-21 21:20:03.905019: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe100 of size 256 next 64\n",
      "2025-08-21 21:20:03.905023: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe200 of size 256 next 65\n",
      "2025-08-21 21:20:03.905027: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe300 of size 256 next 66\n",
      "2025-08-21 21:20:03.905030: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe400 of size 256 next 67\n",
      "2025-08-21 21:20:03.905034: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe500 of size 256 next 68\n",
      "2025-08-21 21:20:03.905038: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe600 of size 256 next 69\n",
      "2025-08-21 21:20:03.905041: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe700 of size 256 next 70\n",
      "2025-08-21 21:20:03.905045: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe800 of size 256 next 71\n",
      "2025-08-21 21:20:03.905049: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fe900 of size 256 next 72\n",
      "2025-08-21 21:20:03.905052: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fea00 of size 256 next 73\n",
      "2025-08-21 21:20:03.905056: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74feb00 of size 256 next 74\n",
      "2025-08-21 21:20:03.905060: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fec00 of size 256 next 75\n",
      "2025-08-21 21:20:03.905063: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fed00 of size 256 next 76\n",
      "2025-08-21 21:20:03.905067: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fee00 of size 256 next 77\n",
      "2025-08-21 21:20:03.905071: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74fef00 of size 256 next 78\n",
      "2025-08-21 21:20:03.905074: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74ff000 of size 256 next 79\n",
      "2025-08-21 21:20:03.905078: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74ff100 of size 256 next 80\n",
      "2025-08-21 21:20:03.905081: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74ff200 of size 256 next 81\n",
      "2025-08-21 21:20:03.905085: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb74ff300 of size 256 next 82\n",
      "2025-08-21 21:20:03.905089: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb74ff400 of size 7994368 next 56\n",
      "2025-08-21 21:20:03.905093: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb7c9f000 of size 8000000 next 55\n",
      "2025-08-21 21:20:03.905096: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440200 of size 512 next 102\n",
      "2025-08-21 21:20:03.905100: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8440400 of size 256 next 175\n",
      "2025-08-21 21:20:03.905104: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440500 of size 256 next 119\n",
      "2025-08-21 21:20:03.905107: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440600 of size 512 next 91\n",
      "2025-08-21 21:20:03.905111: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440800 of size 256 next 92\n",
      "2025-08-21 21:20:03.905115: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8440900 of size 256 next 148\n",
      "2025-08-21 21:20:03.905118: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440a00 of size 256 next 94\n",
      "2025-08-21 21:20:03.905122: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440b00 of size 512 next 97\n",
      "2025-08-21 21:20:03.905126: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440d00 of size 512 next 111\n",
      "2025-08-21 21:20:03.905130: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8440f00 of size 512 next 100\n",
      "2025-08-21 21:20:03.905136: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441100 of size 256 next 112\n",
      "2025-08-21 21:20:03.905140: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8441200 of size 256 next 109\n",
      "2025-08-21 21:20:03.905145: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441300 of size 256 next 106\n",
      "2025-08-21 21:20:03.905150: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8441400 of size 256 next 107\n",
      "2025-08-21 21:20:03.905155: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441500 of size 256 next 122\n",
      "2025-08-21 21:20:03.905160: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441600 of size 256 next 103\n",
      "2025-08-21 21:20:03.905164: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8441700 of size 256 next 118\n",
      "2025-08-21 21:20:03.905169: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441800 of size 256 next 136\n",
      "2025-08-21 21:20:03.905174: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441900 of size 256 next 143\n",
      "2025-08-21 21:20:03.905179: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441a00 of size 256 next 157\n",
      "2025-08-21 21:20:03.905184: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441b00 of size 512 next 98\n",
      "2025-08-21 21:20:03.905188: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441d00 of size 256 next 173\n",
      "2025-08-21 21:20:03.905193: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8441e00 of size 256 next 131\n",
      "2025-08-21 21:20:03.905198: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8441f00 of size 256 next 180\n",
      "2025-08-21 21:20:03.905203: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442000 of size 512 next 158\n",
      "2025-08-21 21:20:03.905208: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8442200 of size 1280 next 90\n",
      "2025-08-21 21:20:03.905212: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442700 of size 256 next 84\n",
      "2025-08-21 21:20:03.905217: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8442800 of size 256 next 196\n",
      "2025-08-21 21:20:03.905222: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442900 of size 256 next 127\n",
      "2025-08-21 21:20:03.905227: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442a00 of size 256 next 128\n",
      "2025-08-21 21:20:03.905232: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442b00 of size 256 next 129\n",
      "2025-08-21 21:20:03.905237: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8442c00 of size 256 next 130\n",
      "2025-08-21 21:20:03.905241: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8442d00 of size 768 next 133\n",
      "2025-08-21 21:20:03.905246: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8443000 of size 256 next 134\n",
      "2025-08-21 21:20:03.905251: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8443100 of size 768 next 137\n",
      "2025-08-21 21:20:03.905256: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8443400 of size 512 next 139\n",
      "2025-08-21 21:20:03.905261: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8443600 of size 256 next 140\n",
      "2025-08-21 21:20:03.905266: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8443700 of size 256 next 141\n",
      "2025-08-21 21:20:03.905271: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8443800 of size 19200 next 154\n",
      "2025-08-21 21:20:03.905276: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb8448300 of size 8192 next 117\n",
      "2025-08-21 21:20:03.905281: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb844a300 of size 256 next 120\n",
      "2025-08-21 21:20:03.905286: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb844a400 of size 256 next 187\n",
      "2025-08-21 21:20:03.905291: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb844a500 of size 256 next 88\n",
      "2025-08-21 21:20:03.905296: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb844a600 of size 29440 next 146\n",
      "2025-08-21 21:20:03.905301: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8451900 of size 876800 next 116\n",
      "2025-08-21 21:20:03.905306: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8527a00 of size 819200 next 159\n",
      "2025-08-21 21:20:03.905311: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb85efa00 of size 1271296 next 101\n",
      "2025-08-21 21:20:03.905316: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb8726000 of size 38400 next 115\n",
      "2025-08-21 21:20:03.905321: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785eb872f600 of size 780800 next 99\n",
      "2025-08-21 21:20:03.905326: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb87ee000 of size 57600 next 114\n",
      "2025-08-21 21:20:03.905331: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb87fc100 of size 819200 next 95\n",
      "2025-08-21 21:20:03.905336: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb88c4100 of size 819200 next 18\n",
      "2025-08-21 21:20:03.905341: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785eb898c100 of size 96000000 next 182\n",
      "2025-08-21 21:20:03.905346: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 785ebe519900 of size 923726336 next 145\n",
      "2025-08-21 21:20:03.905351: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785ef5608b00 of size 8000000 next 151\n",
      "2025-08-21 21:20:03.905356: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785ef5da9d00 of size 1024000000 next 149\n",
      "2025-08-21 21:20:03.905361: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785f32e39d00 of size 1024000000 next 179\n",
      "2025-08-21 21:20:03.905366: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785f6fec9d00 of size 1024000000 next 185\n",
      "2025-08-21 21:20:03.905371: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 785facf59d00 of size 2048000000 next 181\n",
      "2025-08-21 21:20:03.905376: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] InUse at 786027079d00 of size 2048000000 next 171\n",
      "2025-08-21 21:20:03.905381: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1109] Free  at 7860a1199d00 of size 1527997184 next 18446744073709551615\n",
      "2025-08-21 21:20:03.905386: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114]      Summary of in-use Chunks by size: \n",
      "2025-08-21 21:20:03.905393: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 78 Chunks of size 256 totalling 19.5KiB\n",
      "2025-08-21 21:20:03.905399: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 14 Chunks of size 512 totalling 7.0KiB\n",
      "2025-08-21 21:20:03.905405: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 768 totalling 1.5KiB\n",
      "2025-08-21 21:20:03.905410: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-08-21 21:20:03.905416: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 19200 totalling 18.8KiB\n",
      "2025-08-21 21:20:03.905421: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 31744 totalling 31.0KiB\n",
      "2025-08-21 21:20:03.905427: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 36352 totalling 35.5KiB\n",
      "2025-08-21 21:20:03.905433: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 3 Chunks of size 38400 totalling 112.5KiB\n",
      "2025-08-21 21:20:03.905439: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 57600 totalling 112.5KiB\n",
      "2025-08-21 21:20:03.905447: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 76544 totalling 74.8KiB\n",
      "2025-08-21 21:20:03.905452: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 7 Chunks of size 819200 totalling 5.47MiB\n",
      "2025-08-21 21:20:03.905456: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 876800 totalling 856.2KiB\n",
      "2025-08-21 21:20:03.905460: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 1464576 totalling 1.40MiB\n",
      "2025-08-21 21:20:03.905464: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 3 Chunks of size 8000000 totalling 22.89MiB\n",
      "2025-08-21 21:20:03.905469: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 96000000 totalling 91.55MiB\n",
      "2025-08-21 21:20:03.905473: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 3 Chunks of size 1024000000 totalling 2.86GiB\n",
      "2025-08-21 21:20:03.905477: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 2 Chunks of size 2048000000 totalling 3.81GiB\n",
      "2025-08-21 21:20:03.905481: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 3090000128 totalling 2.88GiB\n",
      "2025-08-21 21:20:03.905485: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1117] 1 Chunks of size 6180000000 totalling 5.75GiB\n",
      "2025-08-21 21:20:03.905489: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1121] Sum Total of in-use chunks: 15.43GiB\n",
      "2025-08-21 21:20:03.905493: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1123] Total bytes in pool: 19028312064 memory_limit_: 19028312064 available bytes: 0 curr_region_allocation_bytes_: 38056624128\n",
      "2025-08-21 21:20:03.905499: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Stats: \n",
      "Limit:                     19028312064\n",
      "InUse:                     16566500096\n",
      "MaxInUse:                  17626144256\n",
      "NumAllocs:                      363212\n",
      "MaxAllocSize:               6180000000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-08-21 21:20:03.905506: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:508] **************************************************____**************************************________\n",
      "2025-08-21 21:20:03.905520: W tensorflow/core/framework/op_kernel.cc:1829] RESOURCE_EXHAUSTED: failed to allocate memory\n",
      "2025-08-21 21:20:03.905537: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: RESOURCE_EXHAUSTED: failed to allocate memory\n"
     ],
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling BatchNormalization.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: \u001b[0m\n\nArguments received by BatchNormalization.call():\n  • inputs=tf.Tensor(shape=(16, 500, 500, 128), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResourceExhaustedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Pick a small seed batch for visualization\u001b[39;00m\n\u001b[32m    178\u001b[39m seed_cond = cond_input[:\u001b[32m16\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m generate_and_save_images(generator, seed_cond, epoch+\u001b[32m1\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mgenerate_and_save_images\u001b[39m\u001b[34m(model, cond_input, epoch)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_and_save_images\u001b[39m(model, cond_input, epoch):\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     generated_images = model(cond_input, training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    155\u001b[39m     fig = plt.figure(figsize=(\u001b[32m4\u001b[39m,\u001b[32m4\u001b[39m))\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[32m16\u001b[39m, generated_images.shape[\u001b[32m0\u001b[39m])):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6000\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6001\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6002\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResourceExhaustedError\u001b[39m: Exception encountered when calling BatchNormalization.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:AddV2] name: \u001b[0m\n\nArguments received by BatchNormalization.call():\n  • inputs=tf.Tensor(shape=(16, 500, 500, 128), dtype=float32)\n  • training=False\n  • mask=None"
    
     {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Reference shape: (1030, 500, 500, 3)\n",
      "Texture shape: (1030, 500, 500, 3)\n",
      "Rendered shape: (1030, 500, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython import display\n",
    "\n",
    "# -----------------------\n",
    "# LOAD DATA\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 127.5 - 1.0  # Normalize to [-1,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# CREATE DATASET\n",
    "# -----------------------\n",
    "BATCH_SIZE = 10\n",
    "BUFFER_SIZE = x_ref.shape[0]\n",
    "\n",
    "# Combine ref + texture as conditional input\n",
    "cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((cond_input, x_ren)) \\\n",
    "                               .shuffle(BUFFER_SIZE) \\\n",
    "                               .batch(BATCH_SIZE)\n",
    "\n",
    "# cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "# def data_generator(x, y, batch_size):\n",
    "#     idx = np.arange(len(x))\n",
    "#     while True:\n",
    "#         np.random.shuffle(idx)\n",
    "#         for i in range(0, len(x), batch_size):\n",
    "#             batch_idx = idx[i:i+batch_size]\n",
    "#             yield x[batch_idx], y[batch_idx]\n",
    "\n",
    "# BATCH_SIZE = 10\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_generator(\n",
    "#     lambda: data_generator(cond_input, x_ren, BATCH_SIZE),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(None, 500, 500, 6), dtype=tf.float32),\n",
    "#         tf.TensorSpec(shape=(None, 500, 500, 3), dtype=tf.float32),\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 6)))  # conditional input: ref + texture\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))  # output: RGB\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "# -----------------------\n",
    "# DISCRIMINATOR\n",
    "# -----------------------\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 9)))  # ref + texture + rendered\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# -----------------------\n",
    "# LOSS & OPTIMIZERS\n",
    "# -----------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN STEP\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def train_step(cond_images, real_images):\n",
    "    # cond_images: concatenated reference + texture (B, 500,500,6)\n",
    "    # real_images: rendered images (B,500,500,3)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(cond_images, training=True)\n",
    "\n",
    "        # Discriminator sees concatenated cond + real/fake\n",
    "        real_input = tf.concat([cond_images, real_images], axis=-1)\n",
    "        fake_input = tf.concat([cond_images, generated_images], axis=-1)\n",
    "\n",
    "        real_output = discriminator(real_input, training=True)\n",
    "        fake_output = discriminator(fake_input, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -----------------------\n",
    "# GENERATE & SAVE\n",
    "# -----------------------\n",
    "def generate_and_save_images(model, cond_input, epoch):\n",
    "    generated_images = model(cond_input, training=False)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(min(16, generated_images.shape[0])):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        img = (generated_images[i] + 1.0) / 2.0  # [-1,1] -> [0,1]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('gan/100epch/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for cond_batch, real_batch in train_dataset:\n",
    "        g_loss, d_loss = train_step(cond_batch, real_batch)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    # Pick a small seed batch for visualization\n",
    "    seed_cond = cond_input[:16]\n",
    "    generate_and_save_images(generator, seed_cond, epoch+1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, G loss: {g_loss:.4f}, D loss: {d_loss:.4f}, time: {time.time()-start:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    a = tf.random.normal([1000, 1000])\n",
    "    b = tf.matmul(a, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b18ef879",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Pick a small seed batch for visualization\u001b[39;00m\n\u001b[32m    166\u001b[39m seed_cond = cond_input[:\u001b[32m16\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m generate_and_save_images(generator, seed_cond, epoch+\u001b[32m1\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, G loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, D loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()-start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mgenerate_and_save_images\u001b[39m\u001b[34m(model, cond_input, epoch)\u001b[39m\n\u001b[32m    148\u001b[39m     plt.imshow(img)\n\u001b[32m    149\u001b[39m     plt.axis(\u001b[33m'\u001b[39m\u001b[33moff\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m plt.savefig(\u001b[33m'\u001b[39m\u001b[33mgan/100epch/image_at_epoch_\u001b[39m\u001b[38;5;132;01m{:04d}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m.format(epoch))\n\u001b[32m    151\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/pyplot.py:1243\u001b[39m, in \u001b[36msavefig\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1240\u001b[39m fig = gcf()\n\u001b[32m   1241\u001b[39m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[32m   1242\u001b[39m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m res = fig.savefig(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[32m   1244\u001b[39m fig.canvas.draw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/figure.py:3490\u001b[39m, in \u001b[36mFigure.savefig\u001b[39m\u001b[34m(self, fname, transparent, **kwargs)\u001b[39m\n\u001b[32m   3488\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes:\n\u001b[32m   3489\u001b[39m         _recursively_make_axes_transparent(stack, ax)\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m \u001b[38;5;28mself\u001b[39m.canvas.print_figure(fname, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backend_bases.py:2184\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2181\u001b[39m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[32m   2182\u001b[39m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[32m   2183\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m cbook._setattr_cm(\u001b[38;5;28mself\u001b[39m.figure, dpi=dpi):\n\u001b[32m-> \u001b[39m\u001b[32m2184\u001b[39m         result = print_method(\n\u001b[32m   2185\u001b[39m             filename,\n\u001b[32m   2186\u001b[39m             facecolor=facecolor,\n\u001b[32m   2187\u001b[39m             edgecolor=edgecolor,\n\u001b[32m   2188\u001b[39m             orientation=orientation,\n\u001b[32m   2189\u001b[39m             bbox_inches_restore=_bbox_inches_restore,\n\u001b[32m   2190\u001b[39m             **kwargs)\n\u001b[32m   2191\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backend_bases.py:2040\u001b[39m, in \u001b[36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   2036\u001b[39m     optional_kws = {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[32m   2037\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdpi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfacecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33medgecolor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33morientation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2038\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbbox_inches_restore\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m   2039\u001b[39m     skip = optional_kws - {*inspect.signature(meth).parameters}\n\u001b[32m-> \u001b[39m\u001b[32m2040\u001b[39m     print_method = functools.wraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: meth(\n\u001b[32m   2041\u001b[39m         *args, **{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[32m   2042\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[32m   2043\u001b[39m     print_method = meth\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:481\u001b[39m, in \u001b[36mFigureCanvasAgg.print_png\u001b[39m\u001b[34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, *, metadata=\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    435\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[33;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m \u001b[33;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28mself\u001b[39m._print_pil(filename_or_obj, \u001b[33m\"\u001b[39m\u001b[33mpng\u001b[39m\u001b[33m\"\u001b[39m, pil_kwargs, metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:429\u001b[39m, in \u001b[36mFigureCanvasAgg._print_pil\u001b[39m\u001b[34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    425\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[33;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[32m    427\u001b[39m \u001b[33;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     FigureCanvasAgg.draw(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    430\u001b[39m     mpl.image.imsave(\n\u001b[32m    431\u001b[39m         filename_or_obj, \u001b[38;5;28mself\u001b[39m.buffer_rgba(), \u001b[38;5;28mformat\u001b[39m=fmt, origin=\u001b[33m\"\u001b[39m\u001b[33mupper\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    432\u001b[39m         dpi=\u001b[38;5;28mself\u001b[39m.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:382\u001b[39m, in \u001b[36mFigureCanvasAgg.draw\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.toolbar._wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolbar\n\u001b[32m    381\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28mself\u001b[39m.figure.draw(\u001b[38;5;28mself\u001b[39m.renderer)\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28msuper\u001b[39m().draw()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:94\u001b[39m, in \u001b[36m_finalize_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_wrapper\u001b[39m(artist, renderer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     result = draw(artist, renderer, *args, **kwargs)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m renderer._rasterizing:\n\u001b[32m     96\u001b[39m         renderer.stop_rasterizing()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/figure.py:3257\u001b[39m, in \u001b[36mFigure.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3254\u001b[39m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[32m   3256\u001b[39m     \u001b[38;5;28mself\u001b[39m.patch.draw(renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     mimage._draw_list_compositing_images(\n\u001b[32m   3258\u001b[39m         renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.suppressComposite)\n\u001b[32m   3260\u001b[39m     renderer.close_group(\u001b[33m'\u001b[39m\u001b[33mfigure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/axes/_base.py:3181\u001b[39m, in \u001b[36m_AxesBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[32m   3179\u001b[39m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3181\u001b[39m mimage._draw_list_compositing_images(\n\u001b[32m   3182\u001b[39m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).suppressComposite)\n\u001b[32m   3184\u001b[39m renderer.close_group(\u001b[33m'\u001b[39m\u001b[33maxes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3185\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:599\u001b[39m, in \u001b[36m_ImageBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    597\u001b[39m         renderer.draw_image(gc, l, b, im, trans)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     im, l, b, trans = \u001b[38;5;28mself\u001b[39m.make_image(\n\u001b[32m    600\u001b[39m         renderer, renderer.get_image_magnification())\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    602\u001b[39m         renderer.draw_image(gc, l, b, im)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:902\u001b[39m, in \u001b[36mAxesImage.make_image\u001b[39m\u001b[34m(self, renderer, magnification, unsampled)\u001b[39m\n\u001b[32m    899\u001b[39m transformed_bbox = TransformedBbox(bbox, trans)\n\u001b[32m    900\u001b[39m clip = ((\u001b[38;5;28mself\u001b[39m.get_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes.bbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_clip_on()\n\u001b[32m    901\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).bbox)\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_image(\u001b[38;5;28mself\u001b[39m._A, bbox, transformed_bbox, clip,\n\u001b[32m    903\u001b[39m                         magnification, unsampled=unsampled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:513\u001b[39m, in \u001b[36m_ImageBase._make_image\u001b[39m\u001b[34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[39m\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    511\u001b[39m         output_alpha = _resample(  \u001b[38;5;66;03m# resample alpha channel\u001b[39;00m\n\u001b[32m    512\u001b[39m             \u001b[38;5;28mself\u001b[39m, A[..., \u001b[32m3\u001b[39m], out_shape, t, alpha=alpha)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     output = _resample(  \u001b[38;5;66;03m# resample rgb channels\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28mself\u001b[39m, _rgb_to_rgba(A[..., :\u001b[32m3\u001b[39m]), out_shape, t, alpha=alpha)\n\u001b[32m    515\u001b[39m     output[..., \u001b[32m3\u001b[39m] = output_alpha  \u001b[38;5;66;03m# recombine rgb and alpha\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:210\u001b[39m, in \u001b[36m_resample\u001b[39m\u001b[34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    209\u001b[39m     resample = image_obj.get_resample()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m _image.resample(data, out, transform,\n\u001b[32m    211\u001b[39m                 _interpd_[interpolation],\n\u001b[32m    212\u001b[39m                 resample,\n\u001b[32m    213\u001b[39m                 alpha,\n\u001b[32m    214\u001b[39m                 image_obj.get_filternorm(),\n\u001b[32m    215\u001b[39m                 image_obj.get_filterrad())\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mValueError\u001b[39m: arrays must be of dtype byte, short, float32 or float64"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x73521fe89580> (for post_execute), with arguments args (),kwargs {}:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/pyplot.py:279\u001b[39m, in \u001b[36m_draw_all_if_interactive\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_draw_all_if_interactive\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m matplotlib.is_interactive():\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         draw_all()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/_pylab_helpers.py:131\u001b[39m, in \u001b[36mGcf.draw_all\u001b[39m\u001b[34m(cls, force)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m manager \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.get_all_fig_managers():\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m manager.canvas.figure.stale:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m         manager.canvas.draw_idle()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backend_bases.py:1891\u001b[39m, in \u001b[36mFigureCanvasBase.draw_idle\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_idle_drawing:\n\u001b[32m   1890\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._idle_draw_cntx():\n\u001b[32m-> \u001b[39m\u001b[32m1891\u001b[39m         \u001b[38;5;28mself\u001b[39m.draw(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backends/backend_agg.py:382\u001b[39m, in \u001b[36mFigureCanvasAgg.draw\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.toolbar._wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.toolbar\n\u001b[32m    381\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m     \u001b[38;5;28mself\u001b[39m.figure.draw(\u001b[38;5;28mself\u001b[39m.renderer)\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28msuper\u001b[39m().draw()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:94\u001b[39m, in \u001b[36m_finalize_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_wrapper\u001b[39m(artist, renderer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     result = draw(artist, renderer, *args, **kwargs)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m renderer._rasterizing:\n\u001b[32m     96\u001b[39m         renderer.stop_rasterizing()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/figure.py:3257\u001b[39m, in \u001b[36mFigure.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3254\u001b[39m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[32m   3256\u001b[39m     \u001b[38;5;28mself\u001b[39m.patch.draw(renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     mimage._draw_list_compositing_images(\n\u001b[32m   3258\u001b[39m         renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.suppressComposite)\n\u001b[32m   3260\u001b[39m     renderer.close_group(\u001b[33m'\u001b[39m\u001b[33mfigure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/axes/_base.py:3181\u001b[39m, in \u001b[36m_AxesBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[32m   3179\u001b[39m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3181\u001b[39m mimage._draw_list_compositing_images(\n\u001b[32m   3182\u001b[39m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).suppressComposite)\n\u001b[32m   3184\u001b[39m renderer.close_group(\u001b[33m'\u001b[39m\u001b[33maxes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3185\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:599\u001b[39m, in \u001b[36m_ImageBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    597\u001b[39m         renderer.draw_image(gc, l, b, im, trans)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     im, l, b, trans = \u001b[38;5;28mself\u001b[39m.make_image(\n\u001b[32m    600\u001b[39m         renderer, renderer.get_image_magnification())\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    602\u001b[39m         renderer.draw_image(gc, l, b, im)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:902\u001b[39m, in \u001b[36mAxesImage.make_image\u001b[39m\u001b[34m(self, renderer, magnification, unsampled)\u001b[39m\n\u001b[32m    899\u001b[39m transformed_bbox = TransformedBbox(bbox, trans)\n\u001b[32m    900\u001b[39m clip = ((\u001b[38;5;28mself\u001b[39m.get_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes.bbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_clip_on()\n\u001b[32m    901\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).bbox)\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_image(\u001b[38;5;28mself\u001b[39m._A, bbox, transformed_bbox, clip,\n\u001b[32m    903\u001b[39m                         magnification, unsampled=unsampled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:513\u001b[39m, in \u001b[36m_ImageBase._make_image\u001b[39m\u001b[34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[39m\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    511\u001b[39m         output_alpha = _resample(  \u001b[38;5;66;03m# resample alpha channel\u001b[39;00m\n\u001b[32m    512\u001b[39m             \u001b[38;5;28mself\u001b[39m, A[..., \u001b[32m3\u001b[39m], out_shape, t, alpha=alpha)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     output = _resample(  \u001b[38;5;66;03m# resample rgb channels\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28mself\u001b[39m, _rgb_to_rgba(A[..., :\u001b[32m3\u001b[39m]), out_shape, t, alpha=alpha)\n\u001b[32m    515\u001b[39m     output[..., \u001b[32m3\u001b[39m] = output_alpha  \u001b[38;5;66;03m# recombine rgb and alpha\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:210\u001b[39m, in \u001b[36m_resample\u001b[39m\u001b[34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    209\u001b[39m     resample = image_obj.get_resample()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m _image.resample(data, out, transform,\n\u001b[32m    211\u001b[39m                 _interpd_[interpolation],\n\u001b[32m    212\u001b[39m                 resample,\n\u001b[32m    213\u001b[39m                 alpha,\n\u001b[32m    214\u001b[39m                 image_obj.get_filternorm(),\n\u001b[32m    215\u001b[39m                 image_obj.get_filterrad())\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mValueError\u001b[39m: arrays must be of dtype byte, short, float32 or float64"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must be of dtype byte, short, float32 or float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/IPython/core/formatters.py:402\u001b[39m, in \u001b[36mBaseFormatter.__call__\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m printer(obj)\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[32m    404\u001b[39m method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/IPython/core/pylabtools.py:170\u001b[39m, in \u001b[36mprint_figure\u001b[39m\u001b[34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[32m    168\u001b[39m     FigureCanvasBase(fig)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m fig.canvas.print_figure(bytes_io, **kw)\n\u001b[32m    171\u001b[39m data = bytes_io.getvalue()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fmt == \u001b[33m'\u001b[39m\u001b[33msvg\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/backend_bases.py:2155\u001b[39m, in \u001b[36mFigureCanvasBase.print_figure\u001b[39m\u001b[34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[39m\n\u001b[32m   2152\u001b[39m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[32m   2153\u001b[39m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[32m   2154\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[33m\"\u001b[39m\u001b[33m_draw_disabled\u001b[39m\u001b[33m\"\u001b[39m, nullcontext)():\n\u001b[32m-> \u001b[39m\u001b[32m2155\u001b[39m         \u001b[38;5;28mself\u001b[39m.figure.draw(renderer)\n\u001b[32m   2156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[32m   2157\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches == \u001b[33m\"\u001b[39m\u001b[33mtight\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:94\u001b[39m, in \u001b[36m_finalize_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer, *args, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdraw_wrapper\u001b[39m(artist, renderer, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     result = draw(artist, renderer, *args, **kwargs)\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m renderer._rasterizing:\n\u001b[32m     96\u001b[39m         renderer.stop_rasterizing()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/figure.py:3257\u001b[39m, in \u001b[36mFigure.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3254\u001b[39m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[32m   3256\u001b[39m     \u001b[38;5;28mself\u001b[39m.patch.draw(renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     mimage._draw_list_compositing_images(\n\u001b[32m   3258\u001b[39m         renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.suppressComposite)\n\u001b[32m   3260\u001b[39m     renderer.close_group(\u001b[33m'\u001b[39m\u001b[33mfigure\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3261\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/axes/_base.py:3181\u001b[39m, in \u001b[36m_AxesBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m   3178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[32m   3179\u001b[39m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[32m-> \u001b[39m\u001b[32m3181\u001b[39m mimage._draw_list_compositing_images(\n\u001b[32m   3182\u001b[39m     renderer, \u001b[38;5;28mself\u001b[39m, artists, \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).suppressComposite)\n\u001b[32m   3184\u001b[39m renderer.close_group(\u001b[33m'\u001b[39m\u001b[33maxes\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3185\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:134\u001b[39m, in \u001b[36m_draw_list_compositing_images\u001b[39m\u001b[34m(renderer, parent, artists, suppress_composite)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m         a.draw(renderer)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[32m    137\u001b[39m     image_group = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/artist.py:71\u001b[39m, in \u001b[36mallow_rasterization.<locals>.draw_wrapper\u001b[39m\u001b[34m(artist, renderer)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     69\u001b[39m         renderer.start_filter()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m artist.get_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:599\u001b[39m, in \u001b[36m_ImageBase.draw\u001b[39m\u001b[34m(self, renderer)\u001b[39m\n\u001b[32m    597\u001b[39m         renderer.draw_image(gc, l, b, im, trans)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     im, l, b, trans = \u001b[38;5;28mself\u001b[39m.make_image(\n\u001b[32m    600\u001b[39m         renderer, renderer.get_image_magnification())\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    602\u001b[39m         renderer.draw_image(gc, l, b, im)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:902\u001b[39m, in \u001b[36mAxesImage.make_image\u001b[39m\u001b[34m(self, renderer, magnification, unsampled)\u001b[39m\n\u001b[32m    899\u001b[39m transformed_bbox = TransformedBbox(bbox, trans)\n\u001b[32m    900\u001b[39m clip = ((\u001b[38;5;28mself\u001b[39m.get_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.axes.bbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_clip_on()\n\u001b[32m    901\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_figure(root=\u001b[38;5;28;01mTrue\u001b[39;00m).bbox)\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_image(\u001b[38;5;28mself\u001b[39m._A, bbox, transformed_bbox, clip,\n\u001b[32m    903\u001b[39m                         magnification, unsampled=unsampled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:513\u001b[39m, in \u001b[36m_ImageBase._make_image\u001b[39m\u001b[34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[39m\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    511\u001b[39m         output_alpha = _resample(  \u001b[38;5;66;03m# resample alpha channel\u001b[39;00m\n\u001b[32m    512\u001b[39m             \u001b[38;5;28mself\u001b[39m, A[..., \u001b[32m3\u001b[39m], out_shape, t, alpha=alpha)\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     output = _resample(  \u001b[38;5;66;03m# resample rgb channels\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;28mself\u001b[39m, _rgb_to_rgba(A[..., :\u001b[32m3\u001b[39m]), out_shape, t, alpha=alpha)\n\u001b[32m    515\u001b[39m     output[..., \u001b[32m3\u001b[39m] = output_alpha  \u001b[38;5;66;03m# recombine rgb and alpha\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# output is now either a 2D array of normed (int or float) data\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;66;03m# or an RGBA array of re-sampled input\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/PROJECT/lib/python3.12/site-packages/matplotlib/image.py:210\u001b[39m, in \u001b[36m_resample\u001b[39m\u001b[34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    209\u001b[39m     resample = image_obj.get_resample()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m _image.resample(data, out, transform,\n\u001b[32m    211\u001b[39m                 _interpd_[interpolation],\n\u001b[32m    212\u001b[39m                 resample,\n\u001b[32m    213\u001b[39m                 alpha,\n\u001b[32m    214\u001b[39m                 image_obj.get_filternorm(),\n\u001b[32m    215\u001b[39m                 image_obj.get_filterrad())\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[31mValueError\u001b[39m: arrays must be of dtype byte, short, float32 or float64"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from IPython import display\n",
    "\n",
    "# -----------------------\n",
    "# LOAD DATA\n",
    "# -----------------------\n",
    "def load_images_from_folder(folder, image_size=(500, 500)):\n",
    "    paths = sorted(glob(os.path.join(folder, '*.png')) + glob(os.path.join(folder, '*.jpg')))\n",
    "    images = []\n",
    "    for path in paths:\n",
    "        img = load_img(path, target_size=image_size)\n",
    "        img = img_to_array(img) / 127.5 - 1.0  # Normalize to [-1,1]\n",
    "        images.append(img)\n",
    "    return np.array(images, dtype=np.float32)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "x_ref = load_images_from_folder('dataset/reference')\n",
    "eta_exp = load_images_from_folder('dataset/texture')\n",
    "x_ren = load_images_from_folder('dataset/rendered')\n",
    "\n",
    "print(\"Reference shape:\", x_ref.shape)\n",
    "print(\"Texture shape:\", eta_exp.shape)\n",
    "print(\"Rendered shape:\", x_ren.shape)\n",
    "\n",
    "# -----------------------\n",
    "# CREATE DATASET\n",
    "# -----------------------\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = x_ref.shape[0]\n",
    "\n",
    "# Combine ref + texture as conditional input\n",
    "cond_input = np.concatenate([x_ref, eta_exp], axis=-1)  # shape: (N, 500, 500, 6)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((cond_input, x_ren)) \\\n",
    "                               .shuffle(BUFFER_SIZE) \\\n",
    "                               .batch(BATCH_SIZE)\n",
    "\n",
    "# -----------------------\n",
    "# GENERATOR\n",
    "# -----------------------\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 6)))  # conditional input: ref + texture\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5,5), padding='same'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(3, (5,5), activation='tanh', padding='same'))  # output: RGB\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "# -----------------------\n",
    "# DISCRIMINATOR\n",
    "# -----------------------\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape=(500, 500, 9)))  # ref + texture + rendered\n",
    "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# -----------------------\n",
    "# LOSS & OPTIMIZERS\n",
    "# -----------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN STEP\n",
    "# -----------------------\n",
    "@tf.function\n",
    "def train_step(cond_images, real_images):\n",
    "    # cond_images: concatenated reference + texture (B, 500,500,6)\n",
    "    # real_images: rendered images (B,500,500,3)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(cond_images, training=True)\n",
    "\n",
    "        # Cast generated images to float32\n",
    "        generated_images = tf.cast(generated_images, tf.float32)\n",
    "        \n",
    "        # Discriminator sees concatenated cond + real/fake\n",
    "        real_input = tf.concat([cond_images, real_images], axis=-1)\n",
    "        fake_input = tf.concat([cond_images, generated_images], axis=-1)\n",
    "\n",
    "        real_output = discriminator(real_input, training=True)\n",
    "        fake_output = discriminator(fake_input, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -----------------------\n",
    "# GENERATE & SAVE\n",
    "# -----------------------\n",
    "def generate_and_save_images(model, cond_input, epoch):\n",
    "    generated_images = model(cond_input, training=False)\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "    for i in range(min(16, generated_images.shape[0])):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        img = (generated_images[i] + 1.0) / 2.0  # [-1,1] -> [0,1]\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.savefig('gan/100epch/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------\n",
    "# TRAIN LOOP\n",
    "# -----------------------\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    for cond_batch, real_batch in train_dataset:\n",
    "        g_loss, d_loss = train_step(cond_batch, real_batch)\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    # Pick a small seed batch for visualization\n",
    "    seed_cond = cond_input[:16]\n",
    "    generate_and_save_images(generator, seed_cond, epoch+1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, G loss: {g_loss:.4f}, D loss: {d_loss:.4f}, time: {time.time()-start:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
